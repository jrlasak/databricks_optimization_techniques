{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "extension-intro",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extension: Date-based vs Country Partitioning Comparison\n",
    "\n",
    "This extension notebook demonstrates a concrete implementation of advanced partitioning strategies mentioned in the main project's \"Extending the Lab\" section. We'll create date-partitioned tables alongside country-partitioned ones and compare their scan metrics for different query patterns.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how partition choice affects query performance for different access patterns\n",
    "- Compare directory pruning effectiveness between temporal and categorical partitioning\n",
    "- Learn when to choose date-based vs categorical partitioning\n",
    "- Analyze the trade-offs of multi-dimensional partitioning strategies\n",
    "\n",
    "## Prerequisites\n",
    "Complete the main `project.ipynb` notebook through at least Step 3 (country partitioning) to have baseline data for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "extension-setup",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration - must match main project\n",
    "CATALOG_NAME = \"delta_optimization_project\"\n",
    "SCHEMA_NAME = \"sales_data\"\n",
    "\n",
    "# Extension-specific tables\n",
    "EXTENSION_TABLES = {\n",
    "    \"date_partitioned\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_date_partitioned\",\n",
    "    \"dual_partitioned\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_dual_partitioned\",\n",
    "    \"comparison_results\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.partition_comparison_results\"\n",
    "}\n",
    "\n",
    "# Ensure we're using the right catalog and schema\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "print(f\"✅ Extension environment configured for catalog: {CATALOG_NAME}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "partitioning-strategies",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Partitioning Strategies Implementation\n",
    "\n",
    "We'll create three additional partitioning strategies and compare them:\n",
    "\n",
    "1. **Date-based partitioning** (by sale_date)\n",
    "2. **Dual partitioning** (by country AND year/month)\n",
    "3. **Comparison analysis** with various query patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "create-date-partitioned",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create date-partitioned table\n",
    "def create_date_partitioned_table():\n",
    "    \"\"\"Create a date-partitioned version of the sales data.\"\"\"\n",
    "    \n",
    "    print(\"📅 Creating date-partitioned table...\")\n",
    "    \n",
    "    # Check if source table exists\n",
    "    try:\n",
    "        source_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\").collect()[0]['cnt']\n",
    "        print(f\"   Source table has {source_count:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Source table not found. Please run the main project.ipynb first.\")\n",
    "        return False\n",
    "    \n",
    "    # Create date-partitioned table with year and month partitions\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {EXTENSION_TABLES['date_partitioned']}\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (year, month)\n",
    "    AS \n",
    "    SELECT \n",
    "        *,\n",
    "        YEAR(sale_date) as year,\n",
    "        MONTH(sale_date) as month\n",
    "    FROM {CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\n",
    "    \"\"\")\n",
    "    \n",
    "    # Show partitioning information\n",
    "    print(\"   Date partitioning complete. Partition distribution:\")\n",
    "    partition_dist = spark.sql(f\"\"\"\n",
    "    SELECT year, month, COUNT(*) as row_count\n",
    "    FROM {EXTENSION_TABLES['date_partitioned']}\n",
    "    GROUP BY year, month\n",
    "    ORDER BY year, month\n",
    "    \"\"\")\n",
    "    \n",
    "    display(partition_dist)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Create the date-partitioned table\n",
    "date_table_created = create_date_partitioned_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "create-dual-partitioned",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create dual-partitioned table (country + date)\n",
    "def create_dual_partitioned_table():\n",
    "    \"\"\"Create a table partitioned by both country and date (year-month).\"\"\"\n",
    "    \n",
    "    print(\"🌍📅 Creating dual-partitioned table (country + date)...\")\n",
    "    \n",
    "    # Create dual-partitioned table\n",
    "    # Note: This can create many partitions - in production, be careful about partition explosion\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {EXTENSION_TABLES['dual_partitioned']}\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (country, year_month)\n",
    "    AS \n",
    "    SELECT \n",
    "        *,\n",
    "        CONCAT(YEAR(sale_date), '-', LPAD(MONTH(sale_date), 2, '0')) as year_month\n",
    "    FROM {CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\n",
    "    \"\"\")\n",
    "    \n",
    "    # Analyze partition distribution\n",
    "    print(\"   Dual partitioning complete. Partition distribution:\")\n",
    "    partition_analysis = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        year_month,\n",
    "        COUNT(*) as row_count\n",
    "    FROM {EXTENSION_TABLES['dual_partitioned']}\n",
    "    GROUP BY country, year_month\n",
    "    ORDER BY country, year_month\n",
    "    \"\"\")\n",
    "    \n",
    "    display(partition_analysis)\n",
    "    \n",
    "    # Show partition count warning\n",
    "    total_partitions = partition_analysis.count()\n",
    "    print(f\"   ⚠️ Total partitions created: {total_partitions}\")\n",
    "    if total_partitions > 100:\n",
    "        print(\"   📝 Note: High partition count detected. In production, consider partition consolidation strategies.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Create the dual-partitioned table if date table was successful\n",
    "if date_table_created:\n",
    "    dual_table_created = create_dual_partitioned_table()\n",
    "else:\n",
    "    dual_table_created = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "benchmark-queries",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Benchmark Query Patterns\n",
    "\n",
    "Now let's test different query patterns against our partitioning strategies to see which performs best for different access patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000\     },
     "inputWidgets": {},
     "nuid": "benchmark-framework",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark framework for comparing partitioning strategies\n",
    "class PartitioningBenchmark:\n",
    "    \"\"\"Framework for comparing different partitioning strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        \n",
    "        # Define test queries for different access patterns\n",
    "        self.test_queries = {\n",
    "            \"country_filter\": {\n",
    "                \"description\": \"Filter by single country\",\n",
    "                \"query\": \"SELECT COUNT(*), SUM(amount) FROM {table} WHERE country = 'USA'\",\n",
    "                \"optimal_partition\": \"country\"\n",
    "            },\n",
    "            \"date_range_filter\": {\n",
    "                \"description\": \"Filter by date range\",\n",
    "                \"query\": \"SELECT COUNT(*), AVG(amount) FROM {table} WHERE sale_date >= '2023-06-01' AND sale_date <= '2023-08-31'\",\n",
    "                \"optimal_partition\": \"date\"\n",
    "            },\n",
    "            \"country_and_date\": {\n",
    "                \"description\": \"Filter by country AND date\",\n",
    "                \"query\": \"SELECT product_id, SUM(amount) FROM {table} WHERE country = 'Germany' AND sale_date >= '2023-07-01' AND sale_date <= '2023-07-31' GROUP BY product_id\",\n",
    "                \"optimal_partition\": \"dual\"\n",
    "            },\n",
    "            \"recent_sales\": {\n",
    "                \"description\": \"Recent sales analysis\",\n",
    "                \"query\": \"SELECT country, COUNT(*) FROM {table} WHERE sale_date >= '2023-11-01' GROUP BY country\",\n",
    "                \"optimal_partition\": \"date\"\n",
    "            },\n",
    "            \"customer_country_analysis\": {\n",
    "                \"description\": \"Customer analysis by country\",\n",
    "                \"query\": \"SELECT customer_id, COUNT(*), SUM(amount) FROM {table} WHERE country IN ('USA', 'Canada') GROUP BY customer_id LIMIT 100\",\n",
    "                \"optimal_partition\": \"country\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def run_benchmark(self, table_name, partitioning_strategy, query_name):\n",
    "        \"\"\"Run a single benchmark query and collect metrics.\"\"\"\n",
    "        \n",
    "        query_info = self.test_queries[query_name]\n",
    "        query = query_info[\"query\"].format(table=table_name)\n",
    "        \n",
    "        print(f\"   🔍 Testing: {query_info['description']}\")\n",
    "        \n",
    "        # Execute query with timing\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result_df = spark.sql(query)\n",
    "            row_count = result_df.count()  # Force execution\n",
    "            end_time = time.time()\n",
    "            \n",
    "            duration_ms = int((end_time - start_time) * 1000)\n",
    "            \n",
    "            # Get table metadata\n",
    "            detail_df = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "            detail = detail_df.collect()[0]\n",
    "            num_files = detail['numFiles'] if detail['numFiles'] else 0\n",
    "            \n",
    "            # Record result\n",
    "            result = {\n",
    "                'partitioning_strategy': partitioning_strategy,\n",
    "                'query_name': query_name,\n",
    "                'query_description': query_info['description'],\n",
    "                'optimal_for': query_info['optimal_partition'],\n",
    "                'duration_ms': duration_ms,\n",
    "                'result_rows': row_count,\n",
    "                'total_files': num_files,\n",
    "                'table_name': table_name,\n",
    "                'timestamp': datetime.datetime.now()\n",
    "            }\n",
    "            \n",
    "            self.results.append(result)\n",
    "            \n",
    "            print(f\"      ⏱️ {duration_ms}ms, {row_count} rows, {num_files} files\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ Query failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def run_full_comparison(self):\n",
    "        \"\"\"Run all benchmark queries against all available partitioning strategies.\"\"\"\n",
    "        \n",
    "        tables_to_test = {\n",
    "            \"no_partition\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\",\n",
    "            \"country_partition\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_partitioned\",\n",
    "            \"date_partition\": EXTENSION_TABLES['date_partitioned'],\n",
    "            \"dual_partition\": EXTENSION_TABLES['dual_partitioned']\n",
    "        }\n",
    "        \n",
    "        print(\"🚀 Running comprehensive partitioning benchmark...\\n\")\n",
    "        \n",
    "        for strategy_name, table_name in tables_to_test.items():\n",
    "            print(f\"📊 Testing {strategy_name} strategy ({table_name})\")\n",
    "            \n",
    "            # Check if table exists\n",
    "            try:\n",
    "                spark.sql(f\"SELECT 1 FROM {table_name} LIMIT 1\")\n",
    "            except:\n",
    "                print(f\"   ⚠️ Table {table_name} not found, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Run all queries against this table\n",
    "            for query_name in self.test_queries.keys():\n",
    "                self.run_benchmark(table_name, strategy_name, query_name)\n",
    "            \n",
    "            print()  # Add spacing\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def generate_comparison_report(self):\n",
    "        \"\"\"Generate a comprehensive comparison report.\"\"\"\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"❌ No benchmark results available. Run benchmarks first.\")\n",
    "            return\n",
    "        \n",
    "        # Convert results to DataFrame for analysis\n",
    "        results_df = spark.createDataFrame(self.results)\n",
    "        \n",
    "        print(\"📈 Partitioning Strategy Performance Comparison\\n\")\n",
    "        \n",
    "        # Overall performance by strategy\n",
    "        overall_perf = results_df.groupBy(\"partitioning_strategy\").agg(\n",
    "            F.avg(\"duration_ms\").alias(\"avg_duration_ms\"),\n",
    "            F.min(\"duration_ms\").alias(\"min_duration_ms\"),\n",
    "            F.max(\"duration_ms\").alias(\"max_duration_ms\"),\n",
    "            F.count(\"*\").alias(\"query_count\")\n",
    "        ).orderBy(\"avg_duration_ms\")\n",
    "        \n",
    "        print(\"🏆 Overall Performance Ranking (by average query time):\")\n",
    "        display(overall_perf)\n",
    "        \n",
    "        # Query-specific optimal strategies\n",
    "        print(\"\\n🎯 Best Strategy by Query Type:\")\n",
    "        best_by_query = results_df.select(\n",
    "            \"query_name\", \"query_description\", \"optimal_for\", \n",
    "            \"partitioning_strategy\", \"duration_ms\"\n",
    "        ).orderBy(\"query_name\", \"duration_ms\")\n",
    "        \n",
    "        display(best_by_query)\n",
    "        \n",
    "        # Store results for future analysis\n",
    "        try:\n",
    "            results_df.write.mode(\"overwrite\").saveAsTable(EXTENSION_TABLES['comparison_results'])\n",
    "            print(f\"\\n💾 Results saved to {EXTENSION_TABLES['comparison_results']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Could not save results: {e}\")\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "# Initialize benchmark framework\n",
    "benchmark = PartitioningBenchmark()\n",
    "print(\"✅ Partitioning benchmark framework initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "run-benchmarks",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the comprehensive benchmark comparison\n",
    "if date_table_created and dual_table_created:\n",
    "    benchmark_results = benchmark.run_full_comparison()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BENCHMARK COMPLETE - GENERATING ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    comparison_df = benchmark.generate_comparison_report()\n",
    "else:\n",
    "    print(\"⚠️ Skipping benchmarks - prerequisite tables not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "insights-analysis",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Insights and Recommendations\n",
    "\n",
    "Based on the benchmark results above, here are the key insights about partitioning strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "insights-generator",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate actionable insights from benchmark results\n",
    "def generate_insights():\n",
    "    \"\"\"Generate insights and recommendations from benchmark results.\"\"\"\n",
    "    \n",
    "    if not hasattr(benchmark, 'results') or not benchmark.results:\n",
    "        print(\"📝 Insights based on partitioning theory:\\n\")\n",
    "        \n",
    "        insights = \"\"\"\n",
    "🎯 Expected Performance Patterns:\n",
    "\n",
    "1. Country Partitioning Best For:\n",
    "   • Queries filtering by specific countries\n",
    "   • Regional analysis and reporting\n",
    "   • Customer segmentation by geography\n",
    "   \n",
    "2. Date Partitioning Best For:\n",
    "   • Time-series analysis\n",
    "   • Recent data queries (last N days/months)\n",
    "   • Historical trend analysis\n",
    "   • Data archival and lifecycle management\n",
    "   \n",
    "3. Dual Partitioning Best For:\n",
    "   • Queries with both geographic and temporal filters\n",
    "   • Regional time-series analysis\n",
    "   • Balanced workloads with mixed access patterns\n",
    "   ⚠️ Risk: Partition explosion with too many combinations\n",
    "   \n",
    "📊 General Guidelines:\n",
    "• Choose partitioning based on your most common query patterns\n",
    "• Low-cardinality partitions (10-1000 partitions) work best\n",
    "• Consider Z-Ordering for high-cardinality columns instead of partitioning\n",
    "• Monitor partition sizes - aim for 100MB-1GB per partition\n",
    "• Dual partitioning increases complexity but can improve targeted queries\n",
    "\"\"\"\n",
    "        print(insights)\n",
    "        return\n",
    "    \n",
    "    print(\"📊 PERFORMANCE INSIGHTS FROM BENCHMARK RESULTS\\n\")\n",
    "    \n",
    "    # Find the best performing strategy for each query type\n",
    "    results_df = spark.createDataFrame(benchmark.results)\n",
    "    \n",
    "    best_performers = results_df.select(\n",
    "        \"query_name\", \"query_description\", \"partitioning_strategy\", \"duration_ms\", \"optimal_for\"\n",
    "    ).groupBy(\"query_name\", \"query_description\", \"optimal_for\").agg(\n",
    "        F.min(\"duration_ms\").alias(\"best_time_ms\")\n",
    "    ).join(\n",
    "        results_df,\n",
    "        (F.col(\"query_name\") == results_df.query_name) & \n",
    "        (F.col(\"best_time_ms\") == results_df.duration_ms)\n",
    "    ).select(\n",
    "        F.col(\"query_name\"),\n",
    "        F.col(\"query_description\"), \n",
    "        F.col(\"optimal_for\").alias(\"theory_says_best\"),\n",
    "        results_df.partitioning_strategy.alias(\"actual_best\"),\n",
    "        F.col(\"best_time_ms\")\n",
    "    ).distinct().orderBy(\"query_name\")\n",
    "    \n",
    "    print(\"🏆 Theoretical vs Actual Best Performers:\")\n",
    "    display(best_performers)\n",
    "    \n",
    "    # Calculate improvement ratios\n",
    "    improvement_analysis = results_df.join(\n",
    "        results_df.filter(F.col(\"partitioning_strategy\") == \"no_partition\").select(\n",
    "            F.col(\"query_name\").alias(\"base_query\"),\n",
    "            F.col(\"duration_ms\").alias(\"baseline_ms\")\n",
    "        ),\n",
    "        F.col(\"query_name\") == F.col(\"base_query\")\n",
    "    ).withColumn(\n",
    "        \"improvement_ratio\",\n",
    "        F.round((F.col(\"baseline_ms\") - F.col(\"duration_ms\")) / F.col(\"baseline_ms\") * 100, 1)\n",
    "    ).select(\n",
    "        \"query_name\", \"partitioning_strategy\", \"duration_ms\", \"baseline_ms\", \"improvement_ratio\"\n",
    "    ).orderBy(\"query_name\", F.desc(\"improvement_ratio\"))\n",
    "    \n",
    "    print(\"\\n📈 Performance Improvements vs No Partitioning:\")\n",
    "    display(improvement_analysis)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    print(\"\\n💡 RECOMMENDATIONS:\")\n",
    "    \n",
    "    avg_improvements = improvement_analysis.groupBy(\"partitioning_strategy\").agg(\n",
    "        F.avg(\"improvement_ratio\").alias(\"avg_improvement\")\n",
    "    ).orderBy(F.desc(\"avg_improvement\")).collect()\n",
    "    \n",
    "    if avg_improvements:\n",
    "        best_overall = avg_improvements[0]\n",
    "        print(f\"🥇 Best Overall Strategy: {best_overall['partitioning_strategy']} ({best_overall['avg_improvement']:.1f}% avg improvement)\")\n",
    "        \n",
    "        for i, strategy in enumerate(avg_improvements[:3]):\n",
    "            if i == 0:\n",
    "                print(f\"   • Use for: General purpose workloads with mixed query patterns\")\n",
    "            else:\n",
    "                print(f\"{i+1}. {strategy['partitioning_strategy']}: {strategy['avg_improvement']:.1f}% improvement\")\n",
    "    \n",
    "    print(\"\\n📋 Decision Framework:\")\n",
    "    print(\"• If queries mostly filter by country/region → Use country partitioning\")\n",
    "    print(\"• If queries mostly filter by date ranges → Use date partitioning\")\n",
    "    print(\"• If queries commonly use both filters → Consider dual partitioning (watch partition count!)\")\n",
    "    print(\"• If high-cardinality columns → Use Z-Ordering instead of partitioning\")\n",
    "    print(\"• Monitor partition sizes and query patterns over time\")\n",
    "\n",
    "# Generate insights\n",
    "generate_insights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cleanup-extension",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extension Summary\n",
    "\n",
    "This extension demonstrated:\n",
    "\n",
    "✅ **Concrete Implementation** of suggested lab extensions\n",
    "✅ **Comparative Analysis** of multiple partitioning strategies\n",
    "✅ **Automated Benchmarking** framework for systematic comparison\n",
    "✅ **Data-Driven Insights** based on actual query performance\n",
    "✅ **Practical Recommendations** for partition strategy selection\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Partition choice should align with query patterns** - the most selective filters should drive partitioning decisions\n",
    "2. **Dual partitioning can be powerful** but comes with complexity and partition explosion risks\n",
    "3. **Systematic benchmarking is essential** - theoretical benefits don't always translate to real-world improvements\n",
    "4. **Monitor and iterate** - partition strategies should evolve with changing workload patterns\n",
    "\n",
    "### Integration with Main Project\n",
    "This extension can be run after completing the main `project.ipynb` notebook. It builds upon the foundational concepts and provides a deeper dive into advanced partitioning strategies.\n",
    "\n",
    "### Optional: Clean up extension tables\n",
    "Uncomment and run the cell below to clean up the extension tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "extension-cleanup",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional cleanup - uncomment to remove extension tables\n",
    "# for table_name in EXTENSION_TABLES.values():\n",
    "#     try:\n",
    "#         spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "#         print(f\"✅ Dropped {table_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Could not drop {table_name}: {e}\")\n",
    "\n",
    "print(\"🎓 Extension complete! Check the comparison results and apply insights to your partitioning decisions.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "partitioning_comparison_extension",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}