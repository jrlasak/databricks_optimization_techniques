{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "validation-intro",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake Optimization Project - Validation & Testing\n",
    "\n",
    "This notebook provides validation and testing utilities for the Delta Lake optimization project. Use it to:\n",
    "\n",
    "- ‚úÖ Validate your Databricks environment setup\n",
    "- üîç Test project functionality before running main notebooks\n",
    "- üß™ Verify optimization techniques are working as expected\n",
    "- üìä Quick environment health check\n",
    "\n",
    "## Usage\n",
    "Run this notebook after setting up your environment but before running the main project. It will identify any issues early and provide troubleshooting guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "validation-imports",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries and run basic checks\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "print(\"üîß Delta Lake Optimization Project - Environment Validation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")\n",
    "print(f\"Validation time: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "spark-validation",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate Spark and Delta Lake functionality\n",
    "def validate_spark_environment():\n",
    "    \"\"\"Validate that Spark and Delta Lake are working correctly.\"\"\"\n",
    "    \n",
    "    tests_passed = 0\n",
    "    tests_total = 5\n",
    "    \n",
    "    print(\"üß™ Running Spark and Delta Lake validation tests...\\n\")\n",
    "    \n",
    "    # Test 1: Basic Spark functionality\n",
    "    try:\n",
    "        test_df = spark.range(10).withColumn(\"doubled\", F.col(\"id\") * 2)\n",
    "        row_count = test_df.count()\n",
    "        assert row_count == 10, f\"Expected 10 rows, got {row_count}\"\n",
    "        print(\"‚úÖ Test 1: Basic Spark DataFrame operations\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test 1 FAILED: Basic Spark operations - {e}\")\n",
    "    \n",
    "    # Test 2: Delta Lake table creation\n",
    "    try:\n",
    "        test_table = \"default.validation_test_table\"\n",
    "        test_df = spark.range(5).withColumn(\"test_col\", F.lit(\"test_value\"))\n",
    "        test_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(test_table)\n",
    "        \n",
    "        # Verify table exists and has correct data\n",
    "        read_df = spark.table(test_table)\n",
    "        assert read_df.count() == 5, \"Delta table read/write failed\"\n",
    "        \n",
    "        print(\"‚úÖ Test 2: Delta Lake table creation and reading\")\n",
    "        tests_passed += 1\n",
    "        \n",
    "        # Clean up\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {test_table}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test 2 FAILED: Delta Lake functionality - {e}\")\n",
    "    \n",
    "    # Test 3: DESCRIBE DETAIL command\n",
    "    try:\n",
    "        # Create a temporary table for testing\n",
    "        temp_table = \"default.temp_detail_test\"\n",
    "        spark.range(100).write.format(\"delta\").mode(\"overwrite\").saveAsTable(temp_table)\n",
    "        \n",
    "        detail_df = spark.sql(f\"DESCRIBE DETAIL {temp_table}\")\n",
    "        detail_row = detail_df.collect()[0]\n",
    "        \n",
    "        # Verify we get expected metadata\n",
    "        assert detail_row['numFiles'] is not None, \"numFiles should not be null\"\n",
    "        assert detail_row['sizeInBytes'] is not None, \"sizeInBytes should not be null\"\n",
    "        \n",
    "        print(\"‚úÖ Test 3: DESCRIBE DETAIL metadata extraction\")\n",
    "        tests_passed += 1\n",
    "        \n",
    "        # Clean up\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {temp_table}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test 3 FAILED: DESCRIBE DETAIL functionality - {e}\")\n",
    "    \n",
    "    # Test 4: OPTIMIZE command\n",
    "    try:\n",
    "        # Create table with multiple small files\n",
    "        optimize_table = \"default.optimize_test_table\"\n",
    "        \n",
    "        # Write multiple small partitions to create multiple files\n",
    "        for i in range(3):\n",
    "            spark.range(10).withColumn(\"batch_id\", F.lit(i)).write.format(\"delta\").mode(\"append\").saveAsTable(optimize_table)\n",
    "        \n",
    "        # Run OPTIMIZE\n",
    "        spark.sql(f\"OPTIMIZE {optimize_table}\")\n",
    "        \n",
    "        print(\"‚úÖ Test 4: OPTIMIZE command execution\")\n",
    "        tests_passed += 1\n",
    "        \n",
    "        # Clean up\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {optimize_table}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test 4 FAILED: OPTIMIZE functionality - {e}\")\n",
    "    \n",
    "    # Test 5: Catalog/Schema operations\n",
    "    try:\n",
    "        test_catalog = \"validation_test_catalog\"\n",
    "        test_schema = \"validation_test_schema\"\n",
    "        \n",
    "        # Create and use catalog/schema\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {test_catalog}\")\n",
    "        spark.sql(f\"USE CATALOG {test_catalog}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {test_schema}\")\n",
    "        spark.sql(f\"USE SCHEMA {test_schema}\")\n",
    "        \n",
    "        # Verify context\n",
    "        current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "        current_schema = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "        \n",
    "        assert current_catalog == test_catalog, f\"Expected catalog {test_catalog}, got {current_catalog}\"\n",
    "        assert current_schema == test_schema, f\"Expected schema {test_schema}, got {current_schema}\"\n",
    "        \n",
    "        print(\"‚úÖ Test 5: Catalog and schema operations\")\n",
    "        tests_passed += 1\n",
    "        \n",
    "        # Clean up\n",
    "        spark.sql(f\"DROP CATALOG IF EXISTS {test_catalog} CASCADE\")\n",
    "        spark.sql(\"USE CATALOG main\")  # Reset to default\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test 5 FAILED: Catalog/Schema operations - {e}\")\n",
    "        # Try to reset context even if test failed\n",
    "        try:\n",
    "            spark.sql(\"USE CATALOG main\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nüìä Validation Summary: {tests_passed}/{tests_total} tests passed\")\n",
    "    \n",
    "    if tests_passed == tests_total:\n",
    "        print(\"üéâ All tests passed! Your environment is ready for the Delta Lake optimization project.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {tests_total - tests_passed} tests failed. See troubleshooting section below.\")\n",
    "        return False\n",
    "\n",
    "# Run validation\n",
    "validation_result = validate_spark_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "performance-test",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Performance baseline test\n",
    "def run_performance_baseline():\n",
    "    \"\"\"Run a basic performance test to establish baseline metrics.\"\"\"\n",
    "    \n",
    "    print(\"‚è±Ô∏è  Running performance baseline test...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Create a moderately sized dataset for performance testing\n",
    "        baseline_table = \"default.performance_baseline_test\"\n",
    "        \n",
    "        print(\"   üìä Creating baseline dataset (100K rows)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate synthetic data similar to main project\n",
    "        baseline_df = spark.range(100000).select(\n",
    "            F.col(\"id\").alias(\"row_id\"),\n",
    "            (F.rand() * 1000).cast(\"int\").alias(\"customer_id\"),\n",
    "            (F.rand() * 100).cast(\"int\").alias(\"product_id\"),\n",
    "            (F.rand() * 500 + 10).cast(\"decimal(10,2)\").alias(\"amount\"),\n",
    "            F.expr(\"array('USA', 'Canada', 'UK', 'Germany', 'France')[int(rand() * 5)]\").alias(\"country\"),\n",
    "            F.expr(\"date_add('2023-01-01', int(rand() * 365))\").alias(\"sale_date\")\n",
    "        )\n",
    "        \n",
    "        baseline_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(baseline_table)\n",
    "        create_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Dataset created in {create_time:.2f} seconds\")\n",
    "        \n",
    "        # Test query performance\n",
    "        print(\"   üîç Running baseline query performance test...\")\n",
    "        \n",
    "        query_start = time.time()\n",
    "        result_df = spark.sql(f\"\"\"\n",
    "        SELECT country, \n",
    "               COUNT(*) as total_sales,\n",
    "               AVG(amount) as avg_amount,\n",
    "               SUM(amount) as total_revenue\n",
    "        FROM {baseline_table}\n",
    "        WHERE country IN ('USA', 'Germany')\n",
    "        GROUP BY country\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        result_count = result_df.count()\n",
    "        query_time = time.time() - query_start\n",
    "        \n",
    "        print(f\"   ‚úÖ Query completed in {query_time:.2f} seconds, returned {result_count} rows\")\n",
    "        \n",
    "        # Get table details\n",
    "        detail = spark.sql(f\"DESCRIBE DETAIL {baseline_table}\").collect()[0]\n",
    "        num_files = detail['numFiles'] or 0\n",
    "        size_mb = round((detail['sizeInBytes'] or 0) / 1024 / 1024, 2)\n",
    "        \n",
    "        print(f\"   üìÅ Table: {num_files} files, {size_mb} MB total\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n   üìà Query Results:\")\n",
    "        display(result_df)\n",
    "        \n",
    "        # Performance summary\n",
    "        print(f\"\\nüèÅ Performance Baseline Summary:\")\n",
    "        print(f\"   ‚Ä¢ Data creation: {create_time:.2f}s\")\n",
    "        print(f\"   ‚Ä¢ Query execution: {query_time:.2f}s\")\n",
    "        print(f\"   ‚Ä¢ Files generated: {num_files}\")\n",
    "        print(f\"   ‚Ä¢ Data size: {size_mb} MB\")\n",
    "        \n",
    "        if query_time < 10:\n",
    "            print(\"   ‚úÖ Good baseline performance for optimization experiments\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Slower than expected - may indicate resource constraints\")\n",
    "        \n",
    "        # Clean up\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {baseline_table}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Performance test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run performance test only if validation passed\n",
    "if validation_result:\n",
    "    import time\n",
    "    performance_result = run_performance_baseline()\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping performance test due to validation failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "troubleshooting-guide",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîß Troubleshooting Guide\n",
    "\n",
    "If any validation tests failed, use this guide to resolve common issues:\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### ‚ùå Basic Spark Operations Failed\n",
    "- **Cause**: Spark cluster not properly initialized\n",
    "- **Solution**: Restart your cluster or ensure you're running in a Databricks environment\n",
    "\n",
    "#### ‚ùå Delta Lake Functionality Failed  \n",
    "- **Cause**: Delta Lake not available or not enabled\n",
    "- **Solution**: \n",
    "  - Ensure you're using Databricks Runtime 11.0+ \n",
    "  - For local environments, install delta-spark: `pip install delta-spark`\n",
    "\n",
    "#### ‚ùå DESCRIBE DETAIL Failed\n",
    "- **Cause**: Insufficient permissions or outdated runtime\n",
    "- **Solution**:\n",
    "  - Use Databricks Runtime (not Apache Spark)\n",
    "  - Ensure table access permissions\n",
    "\n",
    "#### ‚ùå OPTIMIZE Command Failed\n",
    "- **Cause**: Command not available or insufficient permissions\n",
    "- **Solution**: \n",
    "  - Use Databricks Runtime with Delta Lake\n",
    "  - Ensure write permissions to the workspace\n",
    "\n",
    "#### ‚ùå Catalog/Schema Operations Failed\n",
    "- **Cause**: Unity Catalog not enabled or insufficient permissions\n",
    "- **Solution**:\n",
    "  - For Free Edition: Use `default` catalog and create schemas only\n",
    "  - For Standard+: Ensure Unity Catalog is configured\n",
    "  - Modify the main project to use existing catalog/schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "final-summary",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final validation summary and next steps\n",
    "print(\"üéì Delta Lake Optimization Project - Validation Complete\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if validation_result:\n",
    "    print(\"‚úÖ Environment validation: PASSED\")\n",
    "    \n",
    "    if 'performance_result' in locals() and performance_result:\n",
    "        print(\"‚úÖ Performance baseline: PASSED\")\n",
    "        print(\"\\nüöÄ Your environment is fully ready!\")\n",
    "        print(\"\\nüìã Next Steps:\")\n",
    "        print(\"1. Open and run project.ipynb - the main learning notebook\")\n",
    "        print(\"2. Try metrics_collection.ipynb for automated tracking\")\n",
    "        print(\"3. Explore partitioning_comparison_extension.ipynb for advanced scenarios\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Performance baseline: FAILED or SKIPPED\")\n",
    "        print(\"\\nüöÄ Environment is functional but may have performance issues\")\n",
    "        print(\"\\nüìã Next Steps:\")\n",
    "        print(\"1. Review cluster configuration (memory, cores)\")\n",
    "        print(\"2. Consider using a larger cluster for better performance\")\n",
    "        print(\"3. Proceed with project.ipynb but expect longer execution times\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Environment validation: FAILED\")\n",
    "    print(\"\\nüîß Action Required:\")\n",
    "    print(\"1. Review the troubleshooting guide above\")\n",
    "    print(\"2. Resolve the failed test issues\")\n",
    "    print(\"3. Re-run this validation notebook\")\n",
    "    print(\"4. Contact support if issues persist\")\n",
    "\n",
    "print(\"\\nüí° Questions or Issues?\")\n",
    "print(\"‚Ä¢ Check README.md for detailed setup instructions\")\n",
    "print(\"‚Ä¢ Review ARCHITECTURE.md for project overview\")\n",
    "print(\"‚Ä¢ Open GitHub Issues for additional support\")\n",
    "\n",
    "print(\"\\nHappy learning! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "validation_and_testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}