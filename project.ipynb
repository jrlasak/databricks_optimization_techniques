{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf1e7c9-ce3f-4ef8-ae77-9a75dd01aab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake Optimization Project\n",
    "\n",
    "This notebook is a guided, hands‑on lab to explore core Delta Lake optimization techniques in Databricks. You'll iteratively: generate data → profile baseline performance → apply optimizations → observe impact in the Spark UI and table metadata.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end you should be able to:\n",
    "- Select appropriate physical layout strategies (partitioning, Z-Ordering, Liquid Clustering)\n",
    "- Diagnose small-file and data-skipping issues\n",
    "- Apply manual vs automatic compaction\n",
    "- Use VACUUM safely for storage hygiene\n",
    "- Read/interpret Spark UI metrics (files scanned, data read, predicate pushdown, scan time)\n",
    "\n",
    "## Flow Overview\n",
    "1. Environment + table name registry\n",
    "2. Data generation (idempotent)\n",
    "3. Baseline query + metrics capture checklist\n",
    "4. Partitioning impact\n",
    "5. Z-Ordering for multi-dimensional skipping\n",
    "6. Manual vs auto compaction\n",
    "7. Liquid Clustering (adaptive layout)\n",
    "8. VACUUM lifecycle management\n",
    "9. Cleanup\n",
    "\n",
    "> Tip: After each transformation, open the Spark UI and record: files scanned, total data read MB, and wall-clock time. Building a mini table of results reinforces the concepts.\n",
    "\n",
    "## Setup\n",
    "We first define the catalog & schema (database) and then a centralized registry of logical table names to keep code DRY and readable. Ensure your workspace permissions allow catalog/schema creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55ad87c-8d1f-4793-bfd0-ec6cc7890111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG_NAME = \"delta_optimization_project\"\n",
    "SCHEMA_NAME = \"sales_data\"\n",
    "\n",
    "# Create the catalog and schema if they don't exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cdf085-bd15-463c-8400-d02724dc4fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Centralized table name registry & helper utilities\n",
    "TABLES = {\n",
    "    \"raw\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\",\n",
    "    \"partitioned\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_partitioned\",\n",
    "    \"raw_zorder\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw_zorder\",\n",
    "    \"to_compact\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_to_compact\",\n",
    "    \"auto_compact\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_auto_compact\",\n",
    "    \"liquid_clustered\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_liquid_clustered\",\n",
    "}\n",
    "\n",
    "def tbl(key: str) -> str:\n",
    "    \"\"\"Return fully qualified table name by logical key.\"\"\"\n",
    "    return TABLES[key]\n",
    "\n",
    "from typing import Optional\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fee3ecd-401f-44a9-853e-dad5239e3076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Generate Synthetic Sales Data\n",
    "\n",
    "We create a moderately large, semi-realistic sales fact dataset to exercise partition pruning and data skipping. Characteristics:\n",
    "- 5M rows (adjustable) to give enough files for optimization to matter\n",
    "- Mixed cardinalities: `country` (low), `customer_id` (medium/high), `product_id` (medium) for contrasting layout strategies\n",
    "- Temporal column `sale_date` enabling potential future partitioning or clustering experiments\n",
    "\n",
    "### Design Notes\n",
    "- Data generation is idempotent: if the base table exists we skip regeneration (saves cluster time)\n",
    "- We intentionally over-partition the initial write (`num_partitions` + low `maxRecordsPerFile`) to create many small files and highlight gains from compaction\n",
    "\n",
    "### Your Tasks\n",
    "1. Run the cell.\n",
    "2. If generation runs, note: number of partitions written, approximate file count (later visible via `DESCRIBE DETAIL` or storage listing).\n",
    "3. If skipped, confirm reuse message printed.\n",
    "\n",
    "Proceed once the table exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb951266-0107-4134-9962-a0d91ad0d06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data at scale using Spark (executor-friendly, no pandas/Faker)\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# If the target table already exists, skip expensive generation and reuse it.\n",
    "# This makes the notebook idempotent for iterative runs.\n",
    "raw_exists = spark.sql(f\"SHOW TABLES IN {CATALOG_NAME}.{SCHEMA_NAME} LIKE 'sales_raw'\").count() > 0\n",
    "if raw_exists:\n",
    "    print(f\"Table {tbl('raw')} already exists; skipping data generation.\")\n",
    "else:\n",
    "    # Number of records to generate (adjust for your cluster)\n",
    "    num_records = 5_000_000\n",
    "\n",
    "    # Controls the number of output files (one file per output partition, roughly).\n",
    "    # Increase partitions to create more small files; decrease to reduce file count.\n",
    "    num_partitions = 1000\n",
    "\n",
    "    # Small list of countries used for sampling (kept on driver only)\n",
    "    countries = ['United States','United Kingdom','Germany','France','Canada','Australia','India','Brazil','Japan','Netherlands']\n",
    "\n",
    "    # Build the DataFrame using Spark primitives (scales across executors)\n",
    "    df = (\n",
    "        spark.range(num_records)\n",
    "        .repartition(num_partitions)\n",
    "        .withColumnRenamed('id', 'seq')\n",
    "        .withColumn(\n",
    "            'transaction_id',\n",
    "            F.concat(\n",
    "                F.lit('txn_'),\n",
    "                F.col('seq').cast('string'),\n",
    "                F.lit('_'),\n",
    "                # cast the double returned by rand() to string before md5 to avoid datatype mismatch\n",
    "                F.substring(F.md5(F.rand(12345).cast('string')), 1, 8)\n",
    "            )\n",
    "        )\n",
    "        .withColumn('customer_id', (F.floor(F.rand(42) * 1001) + 1000).cast('int'))\n",
    "        .withColumn('product_id', (F.floor(F.rand(99) * 401) + 100).cast('int'))\n",
    "        .withColumn('sale_date', F.date_sub(F.current_date(), F.floor(F.rand(7) * 730).cast('int')))\n",
    "        .withColumn('quantity', (F.floor(F.rand(11) * 10) + 1).cast('int'))\n",
    "        .withColumn('unit_price', F.round(F.rand(13) * 190.0 + F.lit(10.5), 2))\n",
    "        .withColumn(\n",
    "            'country',\n",
    "            F.element_at(\n",
    "                F.array(*[F.lit(c) for c in countries]),\n",
    "                (F.floor(F.rand(21) * len(countries)) + 1).cast('int')\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Write the data to a Delta table. Use maxRecordsPerFile to force small files in a performant, distributed write.\n",
    "    (df.write\n",
    "       .format('delta')\n",
    "       .option('maxRecordsPerFile', 5000)\n",
    "       .mode('overwrite')\n",
    "       .saveAsTable(tbl('raw'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e09cc55-284c-49cf-98fb-93b2d6f86220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.table(tbl('raw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed24c04-8d39-40de-b15e-f4eb87caa882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Baseline Query (Unoptimized Table)\n",
    "\n",
    "We establish a performance baseline before altering layout. This isolates the effect of later optimizations.\n",
    "\n",
    "### Query Pattern\n",
    "Filter on a specific `(customer_id, product_id)` pair. This mimics a targeted lookup often seen in downstream analytics or service patterns.\n",
    "\n",
    "### What to Capture (Create a simple log table for yourself)\n",
    "| Metric | Value | How to Find |\n",
    "|--------|-------|-------------|\n",
    "| Files scanned | ? | Spark UI -> SQL/Dataframe -> Scan node |\n",
    "| Bytes read | ? | Spark UI -> Scan details |\n",
    "| Duration (s) | ? | Job / Stage timeline |\n",
    "| Output rows | ? | DataFrame action output |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656aaf49-e263-4406-aeee-ddd5173eb8fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7f6562-c39b-4656-9d0a-83d0a021d677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Baseline query — read from saved Delta table\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_raw.where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3954ff1d-56fb-4a51-998b-91f77ec31743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Open Spark UI while it is/after it finishes.\n",
    "3. Record metrics above.\n",
    "4. Keep this snapshot; you'll compare after each optimization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70376fa4-8432-46c6-9482-3fe1977bd3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Implementing Partitioning\n",
    "\n",
    "Partitioning physically groups data into directory paths by one or more columns. Spark can then prune entire directories early when a query predicate fixes specific partition values, reducing file listing & scan work.\n",
    "\n",
    "### What It Solves\n",
    "- Efficient directory pruning for highly reused, low/moderate-cardinality filters\n",
    "- Can bound per-query file counts for time/region sharded workloads\n",
    "\n",
    "### When to Use\n",
    "- Clear, consistent filtering patterns (e.g. date, region, category)\n",
    "- Low cardinality columns (generally ≲ 1K distinct values across full dataset)\n",
    "- Large tables where pruning meaningfully reduces scan\n",
    "\n",
    "### When NOT to Use\n",
    "- High-cardinality or rapidly growing distinct sets (user ids, fine-grain timestamps)\n",
    "- Workloads without stable filter patterns\n",
    "- Columns that mutate frequently (causes skew / small files)\n",
    "\n",
    "### Trade-offs & Pitfalls\n",
    "- Over-partitioning -> many small files + metadata overhead\n",
    "- Too coarse partition -> limited pruning benefit\n",
    "- Combining with Z-Order: partition first on coarse dimension (e.g. date) then Z-Order inside each partition for additional skipping on high-card columns\n",
    "\n",
    "We choose `country` (~10 values) for demonstration. Our benchmark predicate does not include `country`, so you may see neutral impact—illustrating that partitioning only helps when filters align.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61b8f31-ea60-4ae1-ad0f-330e0a384c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cda5f2b-d52e-4f81-ab6b-a03a0ba4888e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df_raw.write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"country\")\n",
    " .saveAsTable(tbl('partitioned')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6babd604-1382-4007-b535-a77d27f3abc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the same query on the partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f8a113-f8fe-42ce-a3f5-0c0e731a7643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.read.table(tbl('partitioned'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07946c7a-6a57-430e-957d-10b77e25dab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Partitioning:**\n",
    "\n",
    "Go to the Spark UI and compare the execution plan with the baseline. You'll notice that if a filter on the partition key (`country`) was present, Spark would be able to prune entire directories, significantly reducing the amount of data scanned. Even without a direct filter on the partition key, observe any changes in the query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6ddb28-5570-4ddf-8c24-3e3d1f7c1b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Applying Z-Ordering\n",
    "\n",
    "Z-Ordering re-writes data files using a multi-dimensional space-filling curve (Z-order) so rows with similar values for selected columns co-locate. Think of it like organizing a library so related books (rows) live on the same shelf (file) — Spark can then skip whole shelves.\n",
    "\n",
    "### What It Solves\n",
    "- Improves data skipping without exploding partition counts\n",
    "- Helps multi-column filtering patterns (e.g., customer_id AND product_id)\n",
    "- Reduces number of files scanned for selective queries\n",
    "\n",
    "### When to Use\n",
    "- Multi-column filters in your queries\n",
    "- Tables with 2–4 frequently queried, moderate/high-cardinality columns\n",
    "- Data that doesn’t change constantly (batch / micro-batch growth)\n",
    "- Need clustering on columns that would be poor physical partitions\n",
    "\n",
    "### When NOT to Use\n",
    "- Single-column selective queries (simple sorting / partitioning may suffice)\n",
    "- Very frequent small incremental writes (rewrite overhead can outweigh benefit)\n",
    "- More than 4 columns (diminishing returns & larger shuffle)\n",
    "- Columns with extremely high entropy (e.g. UUID) where locality gain is minimal\n",
    "\n",
    "### Column Choice Tips\n",
    "| Scenario | Better Partition | Better Z-Order |\n",
    "|----------|------------------|----------------|\n",
    "| Very low cardinality | Yes | Usually unnecessary |\n",
    "| High cardinality (but queried) | No | Often yes |\n",
    "| Evolving / mixed predicates | Limited | Strong |\n",
    "| Need adaptive maintenance | Sometimes | Consider Liquid Clustering |\n",
    "\n",
    "We Z-Order on `customer_id`, `product_id` to boost skipping for the predicate used in our benchmark query.\n",
    "\n",
    "> Operational Hint: Re-run Z-ORDER after significant (e.g. 10–20%) new data growth in the clustered dimensions or after large backfills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5395cd-c20c-4463-bfa5-e5abb40c0eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create Z-Ordered copy of raw table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e2c337-e291-4f2c-bd22-1b303a0ce6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {tbl('raw_zorder')}\")\n",
    "\n",
    "# Create a separate table from sales_raw to apply Z-Ordering to (keeps sales_partitioned untouched)\n",
    "spark.sql(f\"CREATE TABLE {tbl('raw_zorder')} USING DELTA AS SELECT * FROM {tbl('raw')}\")\n",
    "\n",
    "# Convert the new table to a DeltaTable object\n",
    "delta_table = DeltaTable.forName(spark, tbl('raw_zorder'))\n",
    "\n",
    "# Apply Z-Ordering on high-cardinality columns for the raw copy\n",
    "delta_table.optimize().executeZOrderBy(\"customer_id\", \"product_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e85686-d7b8-4950-885f-2463b4176d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rerun the query to see the effect of Z-Ordering on the z-ordered raw table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71081abd-5829-44f1-9615-f1ae6504645a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.read.table(tbl('raw_zorder'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f8f068-2eb1-4e0c-accd-088aa03d8e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Z-Ordering:**\n",
    "\n",
    "In the Spark UI, look at the \"Details\" for the scan phase. You should see a significant reduction in the number of files read, demonstrating the effectiveness of data skipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12f0bfd-6b53-4cc6-ac13-628fcec8d41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Manual Compaction (OPTIMIZE)\n",
    "\n",
    "Small files inflate job overhead (task scheduling, metadata reads, open/close costs) and can limit effective predicate pushdown (more file headers to scan). `OPTIMIZE` rewrites many small files into fewer larger ones (Databricks default target ~256MB unless overridden) without changing logical content.\n",
    "\n",
    "### What It Solves\n",
    "- Reduces per-task scheduling overhead\n",
    "- Improves scan & filter efficiency by reducing file count\n",
    "- Prepares table for better Z-Ordering / clustering effectiveness\n",
    "\n",
    "### When to Use\n",
    "- Many small files (rule of thumb: lots of files < 100MB)\n",
    "- Bursty or streaming ingestion producing tiny batch outputs\n",
    "- Before heavy analytical / BI workloads to lower latency\n",
    "\n",
    "### When NOT to Use\n",
    "- Files already in healthy size band (~100MB–1GB depending on workload)\n",
    "- Ultra-low latency streaming where rewrite cost is disruptive\n",
    "- Limited compute window / cost constraints (schedule off-peak instead)\n",
    "\n",
    "> Rule of thumb: Target 100–500MB average file size for large analytical tables. Monitor via `DESCRIBE DETAIL` (numFiles & sizeInBytes / numFiles).\n",
    "\n",
    "### Simulation Approach\n",
    "We append tiny random samples repeatedly to manufacture fragmentation, then run `OPTIMIZE` to compact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1ae396-dd98-4970-a8b6-837e08cacc00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run loop to append small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f46e46-c051-492b-b764-aa85e460d615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate small file creation by writing in a loop\n",
    "for i in range(10):\n",
    "    (df_raw.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(tbl('to_compact')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c694f9-9bf1-43f1-9a55-c2fd6a4ce43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "List number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e274cbb-ad3c-4acb-9448-e70568526d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of files\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('to_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b419f69-b7ba-48f4-b0d4-4a33515a0dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Execute `OPTIMIZE` (compaction only — no Z-order here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7f9cb9-d327-4f53-b2d0-c5babe1d6594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now, perform manual compaction\n",
    "delta_table_to_compact = DeltaTable.forName(spark, tbl('to_compact'))\n",
    "delta_table_to_compact.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36711163-356e-4526-b8f5-f39b0617bfb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Re-list object count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46ad8d3-af86-461c-985b-5f7f8440f4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of files\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('to_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d31b79-8287-4595-b596-505b4addbbda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Compaction:**\n",
    "\n",
    "- Observe the reduction in the number of files after running `OPTIMIZE`. This leads to more efficient reads as Spark needs to open and process fewer files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caea28a7-a588-4049-9bf3-2cec0b8040ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Auto Compaction\n",
    "\n",
    "Auto Optimize (optimizeWrite + autoCompact) reduces operational toil: it sizes files during write and schedules asynchronous compaction of lingering small files.\n",
    "\n",
    "### Components\n",
    "- `delta.autoOptimize.optimizeWrite`: coalesces output partitions at commit time\n",
    "- `delta.autoOptimize.autoCompact`: background merge of residual small files after commit\n",
    "\n",
    "### When to Use\n",
    "- Streaming pipelines or frequent micro-batch ingestion\n",
    "- Continuous incremental loads where manual scheduling adds complexity\n",
    "- Teams lacking bandwidth for manual compaction orchestration\n",
    "\n",
    "### When NOT to Use\n",
    "- Large, infrequent batch jobs already producing good-sized files\n",
    "- Cost-sensitive environments (background merges add compute)\n",
    "- Need for bespoke compaction policies / ordering (custom logic required)\n",
    "\n",
    "### Caveats\n",
    "- Background lag: immediate file count may remain high briefly\n",
    "- Not a substitute for Z-Ordering / Liquid Clustering (does not cluster for skipping)\n",
    "- Does not fix skew or suboptimal partition strategy\n",
    "\n",
    "Auto Compaction automatically triggers compaction during writes without manual intervention; you primarily verify its effect via file counts and table history (each micro-batch ideally yields a handful of well-sized files instead of many tiny ones).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3020ea28-6842-4054-bdcd-df5bf9e069a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create table with properties enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0df6bd1-eb9e-413a-aa8e-655ef87009b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {tbl('auto_compact')}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {tbl('auto_compact')}\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "    'delta.autoOptimize.autoCompact' = 'true'\n",
    ")\n",
    "AS SELECT * FROM {tbl('raw')}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of files\n",
    "You can see that the original table contained hundreds/thousands of small files, but the auto_compact table contains now just 1 (if you run this notebook for the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('raw')}\").select(\"numFiles\").show()\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('auto_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the number of rows before performing many small appends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT count(*) FROM {tbl('auto_compact')}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c7275c-e79b-4225-8168-869abd37d0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Perform 70 small appends (approx 50 rows per batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0fdcb1b-e213-4d94-b07b-500615f20d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(70):\n",
    "    (df_raw.sample(fraction=0.00001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(tbl('auto_compact')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows to confirm that new data was appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT count(*) FROM {tbl('auto_compact')}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e597a7-9c34-48a6-8ccf-f609c86cb45c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Auto Compaction:**\n",
    "\n",
    "When you query the table's history after a series of writes, you can see how Delta Lake's auto-optimization features are performing. While you might expect to see OPTIMIZE operations triggered by autoCompact, their absence might actually indicate that the optimizations are working as intended during the write phase.\n",
    "\n",
    "To inspect the history, look at the operation and operationMetrics columns. Notice that each WRITE operation resulted in just one new file (\"numFiles\":\"1\"). This is the work of delta.autoOptimize.optimizeWrite, which efficiently manages file sizes as data is written, preventing the creation of many small files.\n",
    "\n",
    "The autoCompact feature only triggers a separate OPTIMIZE operation if a large number of small files accumulate. Since optimizeWrite is already handling this for your workload, there is no need for a separate compaction step, and that is why you might not see it in the history. However, browse the history and verify it.\n",
    "\n",
    "Background lag: immediate file count after write may still be high briefly. Re-run after a while to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ec5d0d-9f9d-4bea-b33a-66892e0bbd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"DESCRIBE HISTORY {tbl('auto_compact')}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('auto_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dce0814-4dd3-4592-ad37-be363b962a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Liquid Clustering\n",
    "\n",
    "Liquid Clustering dynamically reorganizes data based on query patterns, adapting to your workload without manual tuning. It decouples logical clustering from static partitions — Databricks maintains clustering as data evolves, avoiding rigid directory hierarchies while still enabling data skipping.\n",
    "\n",
    "### Advantages vs Partitioning / Z-Order\n",
    "- Adaptive: automatically reorganizes incremental data\n",
    "- Multi-column \"soft\" clustering without explosion in partitions\n",
    "- Plays well with schema evolution and changing query shapes\n",
    "\n",
    "### When to Use\n",
    "- Evolving query patterns\n",
    "- Multiple clustering columns needed\n",
    "- Want to replace both partitioning and Z-Ordering\n",
    "- High churn / streaming upserts\n",
    "- Many predicates across overlapping column sets\n",
    "- Avoiding maintenance burden of periodic manual Z-Order runs\n",
    "\n",
    "### When NOT to Use\n",
    "- Simple, stable query patterns (a single partition key may be cheaper)\n",
    "- Tables smaller than ~1GB where overhead outweighs benefit\n",
    "- Workloads requiring deterministic physical ordering for downstream consumers\n",
    "\n",
    "> Consider long-term ops: which technique minimizes recurring maintenance for your workload? Liquid Clustering can reduce hands-on maintenance for dynamic workloads but adds some runtime cost and operational monitoring requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01912d9-5503-4777-b2d3-67b72c76a011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create clustered table and load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced3e22f-f5d6-4469-9d67-1c64a410c9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {tbl('liquid_clustered')} (\n",
    "  seq LONG,\n",
    "  transaction_id STRING,\n",
    "  customer_id INT,\n",
    "  product_id INT,\n",
    "  sale_date DATE,\n",
    "  quantity INT,\n",
    "  unit_price DOUBLE,\n",
    "  country STRING\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY (customer_id, product_id)\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into the clustered table\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(tbl('liquid_clustered'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf1f615-a4d7-4819-99b2-453af4e8dab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run predicate query; record scan metrics. Compare with Z-Order table results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1b139c-240d-478b-90bd-2f4cb592e840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rerun the query on the liquid clustered table\n",
    "(spark.read.table(tbl('liquid_clustered'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40780f01-0b38-43ff-8121-c122be60fc22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Liquid Clustering:**\n",
    "\n",
    "1.  Run the query on the liquid clustered table.\n",
    "2.  Examine the Spark UI. Liquid Clustering provides similar data skipping benefits as Z-Ordering but is more adaptive to changes in your data and queries over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c685441-e670-42b6-9f94-bc5df71751f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: VACUUM\n",
    "\n",
    "`VACUUM` removes obsolete files (not referenced by the current Delta log) to reclaim storage & trim metadata. It does NOT rewrite active data; it purges old snapshots beyond retention.\n",
    "\n",
    "### Safety & Retention\n",
    "- Default retention: 7 days (168h) to protect time travel & long-running readers\n",
    "- Never shorten in production unless you fully understand recovery/RPO requirements\n",
    "\n",
    "### When to Use\n",
    "- After major rewrite / delete / update operations producing many stale files\n",
    "- Storage costs are material and retention window already satisfied\n",
    "- Old versions no longer needed for compliance or time travel queries\n",
    "\n",
    "### When NOT to Use\n",
    "- Need historical versions for audits / debugging within retention horizon\n",
    "- Active streaming or batch readers may still reference older snapshots\n",
    "- Immediately after heavy DML when retention window not met\n",
    "\n",
    "> Production Practice: Retain at least 7 days (often 14–30+ for regulated domains). Use table history & storage metrics to justify more frequent vacuuming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0484beba-c8d9-4fff-961e-54842199f71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform updates/deletes before VACUUM to create orphaned files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b12896-a3ce-4358-be4c-bb774bb20e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example update: Increase quantity by 10 for a specific customer\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {tbl('partitioned')}\n",
    "SET quantity = quantity + 10\n",
    "WHERE customer_id >= 1500\n",
    "\"\"\")\n",
    "\n",
    "# Example delete: Remove records for a specific product\n",
    "spark.sql(f\"\"\"\n",
    "DELETE FROM {tbl('partitioned')}\n",
    "WHERE product_id <= 250\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b21fea-26e5-41a9-b357-621164bafdb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check number of files before vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef2f1ce-2402-4528-977c-cd3dd8619e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('partitioned')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5fcba2-00ff-4c5b-aff3-90209bdb89a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run VACUUM\n",
    "\n",
    "> **Note:**  \n",
    "> In Databricks Free Edition, you cannot disable the `retentionDurationCheck` safety feature.  \n",
    "> This means you may need to wait up to **7 days** after performing update or delete operations before orphaned files can be removed by VACUUM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9932fb18-04f7-4eab-ae69-f3ca0a4baf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable the retention check\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "\n",
    "# VACUUM the partitioned table\n",
    "delta_table_to_vacuum = DeltaTable.forName(spark, tbl('partitioned'))\n",
    "delta_table_to_vacuum.vacuum()\n",
    "\n",
    "# Re-enable the retention check\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1e0ad1-f23b-4f02-ae9f-6d4d32cc593a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check number of files after vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6a721d-6bbd-4bbc-a0f0-796953064b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('partitioned')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eb9dbd-3bdd-4d77-b916-90c5e4a76468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of VACUUM:**\n",
    "\n",
    "- The `VACUUM` command cleans up files that are no longer part of the current version of the table. You can check the file system before and after running `VACUUM` on a table that has undergone several modifications to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a2d933-6c9b-4eee-b651-081c3d9771cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Conclusion\n",
    "\n",
    "You applied multiple, complementary Delta optimization techniques and observed their impact empirically.\n",
    "\n",
    "(See preceding section: Common Mistakes & Anti-Patterns for operational guardrails.)\n",
    "\n",
    "### Comparative Summary (Qualitative)\n",
    "| Technique | Primary Benefit | Ideal Use Case | Ongoing Maintenance |\n",
    "|-----------|-----------------|----------------|---------------------|\n",
    "| Partitioning | Directory pruning | Low-cardinality, common filters | Low once chosen |\n",
    "| Z-Ordering | Multi-column data skipping | Repeated predicates across moderate/high-card columns | Periodic re-run as data grows |\n",
    "| Manual Compaction | Fewer larger files | Backfill / bursty small writes | Run as needed (monitor file size dist) |\n",
    "| Auto Optimize | Reduce small files proactively | Continuous ingestion workloads | Minimal |\n",
    "| Liquid Clustering | Adaptive layout | Dynamic predicates & evolving schemas | Managed (monitor clustering metrics) |\n",
    "| VACUUM | Storage reclamation | Any table with churn | Scheduled (respect retention) |\n",
    "\n",
    "### Key Heuristics\n",
    "- Start simple: partition only when clear benefit\n",
    "- Use Z-Order or Liquid Clustering to refine skipping without over-partitioning\n",
    "- Monitor file counts & avg file size (DESCRIBE DETAIL) for compaction signals\n",
    "- Track clustering freshness (last Z-Order or clustering maintenance)\n",
    "- Align time travel retention with recovery & audit requirements\n",
    "\n",
    "### Suggested Next Experiments\n",
    "1. Add a date-based partition layer and compare with country partitioning\n",
    "2. Introduce updates/deletes & measure impact on small file generation\n",
    "3. Benchmark query patterns with and without Z-Order refresh after significant data growth\n",
    "4. Use Photon runtime and compare scan metrics\n",
    "5. Track metrics programmatically and plot improvements over time\n",
    "\n",
    "## Cleanup\n",
    "Run the following cell to drop the catalog and all associated tables created during this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Mistakes & Anti-Patterns\n",
    "\n",
    "1. Over-partitioning modest datasets (creates thousands of tiny directories & files)\n",
    "2. Z-Ordering on too many columns (shuffle blow-up; diminishing returns >4)\n",
    "3. Skipping VACUUM in cost-sensitive environments (latent storage bloat)\n",
    "4. Using Liquid Clustering on tiny tables (<1GB) where overhead > benefit\n",
    "5. Running manual OPTIMIZE on tables already governed by auto compaction (duplicate cost)\n",
    "6. Z-Ordering on extremely high-cardinality random IDs / high-precision timestamps (low locality gain)\n",
    "7. Not monitoring average file size drift after optimizations (missing 100MB–1GB target band)\n",
    "8. Blanket applying the same strategy to every table without considering access patterns\n",
    "9. Forgetting to re-run Z-Order after large backfills (stale clustering)\n",
    "10. Lowering VACUUM retention below recovery requirements (operational risk)\n",
    "\n",
    "> Recommendation: Maintain a lightweight observability sheet (table, avg file size, numFiles, last OPTIMIZE/Z-ORDER run, retention) to proactively detect drift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4afdf617-a65a-44ad-852a-0783313a446c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the catalog and all its contents\n",
    "# spark.sql(f\"DROP CATALOG IF EXISTS {CATALOG_NAME} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
