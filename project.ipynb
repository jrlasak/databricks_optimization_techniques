{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf1e7c9-ce3f-4ef8-ae77-9a75dd01aab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake Optimization Project\n",
    "\n",
    "This notebook is a guided, hands‑on lab to explore core Delta Lake optimization techniques in Databricks. You'll iteratively: generate data → profile baseline performance → apply optimizations → observe impact in the Spark UI and table metadata.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end you should be able to:\n",
    "- Select appropriate physical layout strategies (partitioning, Z-Ordering, Liquid Clustering)\n",
    "- Diagnose small-file and data-skipping issues\n",
    "- Apply manual vs automatic compaction\n",
    "- Use VACUUM safely for storage hygiene\n",
    "- Read/interpret Spark UI metrics (files scanned, data read, predicate pushdown, scan time)\n",
    "\n",
    "## Flow Overview\n",
    "1. Environment + table name registry\n",
    "2. Data generation (idempotent)\n",
    "3. Baseline query + metrics capture checklist\n",
    "4. Partitioning impact\n",
    "5. Z-Ordering for multi-dimensional skipping\n",
    "6. Manual vs auto compaction\n",
    "7. Liquid Clustering (adaptive layout)\n",
    "8. VACUUM lifecycle management\n",
    "9. Cleanup\n",
    "\n",
    "> Tip: After each transformation, open the Spark UI and record: files scanned, total data read MB, and wall-clock time. Building a mini table of results reinforces the concepts.\n",
    "\n",
    "## Setup\n",
    "We first define the catalog & schema (database) and then a centralized registry of logical table names to keep code DRY and readable. Ensure your workspace permissions allow catalog/schema creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55ad87c-8d1f-4793-bfd0-ec6cc7890111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG_NAME = \"delta_optimization_project\"\n",
    "SCHEMA_NAME = \"sales_data\"\n",
    "\n",
    "# Create the catalog and schema if they don't exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cdf085-bd15-463c-8400-d02724dc4fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Centralized table name registry & helper utilities\n",
    "TABLES = {\n",
    "    \"raw\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\",\n",
    "    \"partitioned\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_partitioned\",\n",
    "    \"raw_zorder\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw_zorder\",\n",
    "    \"to_compact\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_to_compact\",\n",
    "    \"auto_compact\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_auto_compact\",\n",
    "    \"liquid_clustered\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_liquid_clustered\",\n",
    "}\n",
    "\n",
    "def tbl(key: str) -> str:\n",
    "    \"\"\"Return fully qualified table name by logical key.\"\"\"\n",
    "    return TABLES[key]\n",
    "\n",
    "from typing import Optional\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fee3ecd-401f-44a9-853e-dad5239e3076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Generate Synthetic Sales Data\n",
    "\n",
    "We create a moderately large, semi-realistic sales fact dataset to exercise partition pruning and data skipping. Characteristics:\n",
    "- 5M rows (adjustable) to give enough files for optimization to matter\n",
    "- Mixed cardinalities: `country` (low), `customer_id` (medium/high), `product_id` (medium) for contrasting layout strategies\n",
    "- Temporal column `sale_date` enabling potential future partitioning or clustering experiments\n",
    "\n",
    "### Design Notes\n",
    "- Data generation is idempotent: if the base table exists we skip regeneration (saves cluster time)\n",
    "- We intentionally over-partition the initial write (`num_partitions` + low `maxRecordsPerFile`) to create many small files and highlight gains from compaction\n",
    "\n",
    "### Your Tasks\n",
    "1. Run the cell.\n",
    "2. If generation runs, note: number of partitions written, approximate file count (later visible via `DESCRIBE DETAIL` or storage listing).\n",
    "3. If skipped, confirm reuse message printed.\n",
    "\n",
    "Proceed once the table exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb951266-0107-4134-9962-a0d91ad0d06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data at scale using Spark (executor-friendly, no pandas/Faker)\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# If the target table already exists, skip expensive generation and reuse it.\n",
    "# This makes the notebook idempotent for iterative runs.\n",
    "raw_exists = spark.sql(f\"SHOW TABLES IN {CATALOG_NAME}.{SCHEMA_NAME} LIKE 'sales_raw'\").count() > 0\n",
    "if raw_exists:\n",
    "    print(f\"Table {tbl('raw')} already exists; skipping data generation.\")\n",
    "else:\n",
    "    # Number of records to generate (adjust for your cluster)\n",
    "    num_records = 5_000_000\n",
    "\n",
    "    # Controls the number of output files (one file per output partition, roughly).\n",
    "    # Increase partitions to create more small files; decrease to reduce file count.\n",
    "    num_partitions = 1000\n",
    "\n",
    "    # Small list of countries used for sampling (kept on driver only)\n",
    "    countries = ['United States','United Kingdom','Germany','France','Canada','Australia','India','Brazil','Japan','Netherlands']\n",
    "\n",
    "    # Build the DataFrame using Spark primitives (scales across executors)\n",
    "    df = (\n",
    "        spark.range(num_records)\n",
    "        .repartition(num_partitions)\n",
    "        .withColumnRenamed('id', 'seq')\n",
    "        .withColumn(\n",
    "            'transaction_id',\n",
    "            F.concat(\n",
    "                F.lit('txn_'),\n",
    "                F.col('seq').cast('string'),\n",
    "                F.lit('_'),\n",
    "                # cast the double returned by rand() to string before md5 to avoid datatype mismatch\n",
    "                F.substring(F.md5(F.rand(12345).cast('string')), 1, 8)\n",
    "            )\n",
    "        )\n",
    "        .withColumn('customer_id', (F.floor(F.rand(42) * 1001) + 1000).cast('int'))\n",
    "        .withColumn('product_id', (F.floor(F.rand(99) * 401) + 100).cast('int'))\n",
    "        .withColumn('sale_date', F.date_sub(F.current_date(), F.floor(F.rand(7) * 730).cast('int')))\n",
    "        .withColumn('quantity', (F.floor(F.rand(11) * 10) + 1).cast('int'))\n",
    "        .withColumn('unit_price', F.round(F.rand(13) * 190.0 + F.lit(10.5), 2))\n",
    "        .withColumn(\n",
    "            'country',\n",
    "            F.element_at(\n",
    "                F.array(*[F.lit(c) for c in countries]),\n",
    "                (F.floor(F.rand(21) * len(countries)) + 1).cast('int')\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Write the data to a Delta table. Use maxRecordsPerFile to force small files in a performant, distributed write.\n",
    "    (df.write\n",
    "       .format('delta')\n",
    "       .option('maxRecordsPerFile', 5000)\n",
    "       .mode('overwrite')\n",
    "       .saveAsTable(tbl('raw'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e09cc55-284c-49cf-98fb-93b2d6f86220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.table(tbl('raw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed24c04-8d39-40de-b15e-f4eb87caa882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Baseline Query (Unoptimized Table)\n",
    "\n",
    "We establish a performance baseline before altering layout. This isolates the effect of later optimizations.\n",
    "\n",
    "### Query Pattern\n",
    "Filter on a specific `(customer_id, product_id)` pair. This mimics a targeted lookup often seen in downstream analytics or service patterns.\n",
    "\n",
    "### What to Capture (Create a simple log table for yourself)\n",
    "| Metric | Value | How to Find |\n",
    "|--------|-------|-------------|\n",
    "| Files scanned | ? | Spark UI -> SQL/Dataframe -> Scan node |\n",
    "| Bytes read | ? | Spark UI -> Scan details |\n",
    "| Duration (s) | ? | Job / Stage timeline |\n",
    "| Output rows | ? | DataFrame action output |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656aaf49-e263-4406-aeee-ddd5173eb8fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7f6562-c39b-4656-9d0a-83d0a021d677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Baseline query — read from saved Delta table\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_raw.where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3954ff1d-56fb-4a51-998b-91f77ec31743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Open Spark UI while it is/after it finishes.\n",
    "3. Record metrics above.\n",
    "4. Keep this snapshot; you'll compare after each optimization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70376fa4-8432-46c6-9482-3fe1977bd3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Implementing Partitioning\n",
    "\n",
    "Partitioning physically groups data by directory on chosen columns. Pros: efficient directory pruning when filters match partition predicates. Cons: too many partitions => small files & metadata overhead; high-cardinality partition keys are an anti-pattern.\n",
    "\n",
    "We choose `country` (low cardinality ~10) as a demonstrative partition column. Even though our query predicate currently does *not* include `country`, observing the plan difference illustrates when partitioning helps or is neutral.\n",
    "\n",
    "### Good Candidates\n",
    "- Low to moderate cardinality (10s–100s distinct) used frequently in filters\n",
    "- Dimension-style attributes (e.g., region, date (at coarse grain), category)\n",
    "\n",
    "### Avoid\n",
    "- High-cardinality (user ids, transaction ids)\n",
    "- Rapidly growing distinct sets (timestamps to the second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61b8f31-ea60-4ae1-ad0f-330e0a384c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cda5f2b-d52e-4f81-ab6b-a03a0ba4888e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df_raw.write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"country\")\n",
    " .saveAsTable(tbl('partitioned')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6babd604-1382-4007-b535-a77d27f3abc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the same query on the partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f8a113-f8fe-42ce-a3f5-0c0e731a7643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.read.table(tbl('partitioned'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07946c7a-6a57-430e-957d-10b77e25dab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Partitioning:**\n",
    "\n",
    "Go to the Spark UI and compare the execution plan with the baseline. You'll notice that if a filter on the partition key (`country`) was present, Spark would be able to prune entire directories, significantly reducing the amount of data scanned. Even without a direct filter on the partition key, observe any changes in the query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6ddb28-5570-4ddf-8c24-3e3d1f7c1b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Applying Z-Ordering\n",
    "\n",
    "Z-Ordering re-writes data files using a multi-dimensional space-filling curve (Z-order) so rows with similar values for selected columns co-locate. This increases data skipping efficiency without exploding directory counts like over-partitioning.\n",
    "\n",
    "### When to Use\n",
    "- Repeated predicates on 1–4 moderate/high cardinality columns\n",
    "- Mixed access patterns (can't commit to a single partition key)\n",
    "- Need to cluster on columns that would be poor partitions\n",
    "\n",
    "### Column Choice Tips\n",
    "| Scenario | Better for Partition | Better for Z-Order |\n",
    "|----------|---------------------|--------------------|\n",
    "| Very low cardinality | Yes | Unnecessary |\n",
    "| High cardinality | No | Often yes |\n",
    "| Evolving query set | Maybe (limited) | Yes |\n",
    "\n",
    "We Z-Order on `customer_id`, `product_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5395cd-c20c-4463-bfa5-e5abb40c0eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create Z-Ordered copy of raw table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e2c337-e291-4f2c-bd22-1b303a0ce6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {tbl('raw_zorder')}\")\n",
    "\n",
    "# Create a separate table from sales_raw to apply Z-Ordering to (keeps sales_partitioned untouched)\n",
    "spark.sql(f\"CREATE TABLE {tbl('raw_zorder')} USING DELTA AS SELECT * FROM {tbl('raw')}\")\n",
    "\n",
    "# Convert the new table to a DeltaTable object\n",
    "delta_table = DeltaTable.forName(spark, tbl('raw_zorder'))\n",
    "\n",
    "# Apply Z-Ordering on high-cardinality columns for the raw copy\n",
    "delta_table.optimize().executeZOrderBy(\"customer_id\", \"product_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e85686-d7b8-4950-885f-2463b4176d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rerun the query to see the effect of Z-Ordering on the z-ordered raw table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71081abd-5829-44f1-9615-f1ae6504645a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.read.table(tbl('raw_zorder'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f8f068-2eb1-4e0c-accd-088aa03d8e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Z-Ordering:**\n",
    "\n",
    "In the Spark UI, look at the \"Details\" for the scan phase. You should see a significant reduction in the number of files read, demonstrating the effectiveness of data skipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12f0bfd-6b53-4cc6-ac13-628fcec8d41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Manual Compaction (OPTIMIZE)\n",
    "\n",
    "Small files inflate job overhead (task scheduling, metadata reads, open/close costs) and degrade predicate pushdown (more file headers to scan). `OPTIMIZE` rewrites many small files into fewer larger ones (typically 256MB target on Databricks unless overridden).\n",
    "\n",
    "### Simulation\n",
    "We append tiny random samples repeatedly to manufacture fragmentation.\n",
    "\n",
    "> Rule of thumb: Target 100–500MB per file for large analytical tables (depends on workload)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1ae396-dd98-4970-a8b6-837e08cacc00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run loop to append small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f46e46-c051-492b-b764-aa85e460d615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate small file creation by writing in a loop\n",
    "for i in range(10):\n",
    "    (df_raw.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(tbl('to_compact')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c694f9-9bf1-43f1-9a55-c2fd6a4ce43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "List number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e274cbb-ad3c-4acb-9448-e70568526d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of files\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('to_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b419f69-b7ba-48f4-b0d4-4a33515a0dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Execute `OPTIMIZE` (compaction only — no Z-order here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7f9cb9-d327-4f53-b2d0-c5babe1d6594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now, perform manual compaction\n",
    "delta_table_to_compact = DeltaTable.forName(spark, tbl('to_compact'))\n",
    "delta_table_to_compact.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36711163-356e-4526-b8f5-f39b0617bfb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Re-list object count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46ad8d3-af86-461c-985b-5f7f8440f4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of files\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('to_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d31b79-8287-4595-b596-505b4addbbda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Compaction:**\n",
    "\n",
    "- Observe the reduction in the number of files after running `OPTIMIZE`. This leads to more efficient reads as Spark needs to open and process fewer files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caea28a7-a588-4049-9bf3-2cec0b8040ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Auto Compaction\n",
    "\n",
    "Auto Optimize (optimizeWrites + autoCompact) reduces operational toil. It sizes files during write and schedules asynchronous compaction of lingering small files.\n",
    "\n",
    "### What It Does\n",
    "- optimizeWrite: coalesces output at write time to hit target size\n",
    "- autoCompact: background job merges residual small files post-commit\n",
    "\n",
    "### Caveats\n",
    "- Not a substitute for strategic Z-Ordering or Liquid Clustering\n",
    "- Background lag: immediate file count after write may still be high briefly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3020ea28-6842-4054-bdcd-df5bf9e069a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create table with properties enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0df6bd1-eb9e-413a-aa8e-655ef87009b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {tbl('auto_compact')}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {tbl('auto_compact')}\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "    'delta.autoOptimize.autoCompact' = 'true'\n",
    ")\n",
    "AS SELECT * FROM {tbl('raw')}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of files\n",
    "You can see that the original table contained hundreds/thousands of small files, but the auto_compact table contains now just 1 (if you run this notebook for the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('raw')}\").select(\"numFiles\").show()\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('auto_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the number of rows before performing many small appends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT count(*) FROM {tbl('auto_compact')}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c7275c-e79b-4225-8168-869abd37d0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Perform 70 small appends (approx 50 rows per batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0fdcb1b-e213-4d94-b07b-500615f20d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(70):\n",
    "    (df_raw.sample(fraction=0.00001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(tbl('auto_compact')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows to confirm that new data was appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT count(*) FROM {tbl('auto_compact')}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e597a7-9c34-48a6-8ccf-f609c86cb45c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Auto Compaction:**\n",
    "\n",
    "When you query the table's history after a series of writes, you can see how Delta Lake's auto-optimization features are performing. While you might expect to see OPTIMIZE operations triggered by autoCompact, their absence in this case actually indicates that the optimizations are working as intended during the write phase.\n",
    "\n",
    "To inspect the history, look at the operation and operationMetrics columns. Notice that each WRITE operation resulted in just one new file (\"numFiles\":\"1\"). This is the work of delta.autoOptimize.optimizeWrite, which efficiently manages file sizes as data is written, preventing the creation of many small files.\n",
    "\n",
    "The autoCompact feature only triggers a separate OPTIMIZE operation if a large number of small files accumulate. Since optimizeWrite is already handling this for your workload, there is no need for a separate compaction step, and that is why you might not see it in the history.\n",
    "\n",
    "Background lag: immediate file count after write may still be high briefly. Re-run after a while to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ec5d0d-9f9d-4bea-b33a-66892e0bbd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"DESCRIBE HISTORY {tbl('auto_compact')}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('auto_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dce0814-4dd3-4592-ad37-be363b962a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Liquid Clustering\n",
    "\n",
    "Liquid Clustering decouples logical clustering from static partitions. Databricks can dynamically maintain clustering as data evolves without rigid directory hierarchies.\n",
    "\n",
    "### Advantages vs Partitioning/Z-Order\n",
    "- Adaptive: automatically reorganizes incremental data\n",
    "- Multi-column \"soft\" clustering without explosion in partitions\n",
    "- Plays well with schema evolution and changing query shapes\n",
    "\n",
    "### When to Prefer\n",
    "- High churn / streaming upserts\n",
    "- Many predicates across overlapping column sets\n",
    "- Avoiding maintenance burden of periodic manual Z-Order runs\n",
    "\n",
    "> Consider long-term ops: Which technique minimizes recurring maintenance for your workload?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01912d9-5503-4777-b2d3-67b72c76a011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create clustered table and load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced3e22f-f5d6-4469-9d67-1c64a410c9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {tbl('liquid_clustered')} (\n",
    "  seq LONG,\n",
    "  transaction_id STRING,\n",
    "  customer_id INT,\n",
    "  product_id INT,\n",
    "  sale_date DATE,\n",
    "  quantity INT,\n",
    "  unit_price DOUBLE,\n",
    "  country STRING\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY (customer_id, product_id)\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into the clustered table\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(tbl('liquid_clustered'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf1f615-a4d7-4819-99b2-453af4e8dab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run predicate query; record scan metrics. Compare with Z-Order table results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1b139c-240d-478b-90bd-2f4cb592e840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rerun the query on the liquid clustered table\n",
    "(spark.read.table(tbl('liquid_clustered'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40780f01-0b38-43ff-8121-c122be60fc22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Liquid Clustering:**\n",
    "\n",
    "1.  Run the query on the liquid clustered table.\n",
    "2.  Examine the Spark UI. Liquid Clustering provides similar data skipping benefits as Z-Ordering but is more adaptive to changes in your data and queries over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c685441-e670-42b6-9f94-bc5df71751f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: VACUUM\n",
    "\n",
    "`VACUUM` removes obsolete files no longer referenced by the Delta transaction log (old snapshots, overwritten data). This reclaims storage and can reduce metadata overhead.\n",
    "\n",
    "### Safety Mechanism\n",
    "Default retention (168h / 7d) protects against readers holding older snapshots (time travel) and against eventual consistency edge cases.\n",
    "\n",
    "### Demo Adjustment\n",
    "We disable retention checks only for demonstration to show effect immediately — *never do this in production* unless you fully understand snapshot isolation & streaming lag impacts.\n",
    "\n",
    "> Production Practice: Keep retention ≥ 7 days; align with compliance & restore objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0484beba-c8d9-4fff-961e-54842199f71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform updates/deletes before VACUUM to create orphaned files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b12896-a3ce-4358-be4c-bb774bb20e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example update: Increase quantity by 10 for a specific customer\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {tbl('partitioned')}\n",
    "SET quantity = quantity + 10\n",
    "WHERE customer_id >= 1500\n",
    "\"\"\")\n",
    "\n",
    "# Example delete: Remove records for a specific product\n",
    "spark.sql(f\"\"\"\n",
    "DELETE FROM {tbl('partitioned')}\n",
    "WHERE product_id <= 250\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b21fea-26e5-41a9-b357-621164bafdb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check number of files before vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef2f1ce-2402-4528-977c-cd3dd8619e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('partitioned')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5fcba2-00ff-4c5b-aff3-90209bdb89a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run VACUUM\n",
    "\n",
    "> **Note:**  \n",
    "> In Databricks Free Edition, you cannot disable the `retentionDurationCheck` safety feature.  \n",
    "> This means you may need to wait up to **7 days** after performing update or delete operations before orphaned files can be removed by VACUUM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9932fb18-04f7-4eab-ae69-f3ca0a4baf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable the retention check\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "\n",
    "# VACUUM the partitioned table\n",
    "delta_table_to_vacuum = DeltaTable.forName(spark, tbl('partitioned'))\n",
    "delta_table_to_vacuum.vacuum()\n",
    "\n",
    "# Re-enable the retention check\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1e0ad1-f23b-4f02-ae9f-6d4d32cc593a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check number of files after vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6a721d-6bbd-4bbc-a0f0-796953064b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('partitioned')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eb9dbd-3bdd-4d77-b916-90c5e4a76468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of VACUUM:**\n",
    "\n",
    "- The `VACUUM` command cleans up files that are no longer part of the current version of the table. You can check the file system before and after running `VACUUM` on a table that has undergone several modifications to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a2d933-6c9b-4eee-b651-081c3d9771cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Conclusion\n",
    "\n",
    "You applied multiple, complementary Delta optimization techniques and observed their impact empirically.\n",
    "\n",
    "### Comparative Summary (Qualitative)\n",
    "| Technique | Primary Benefit | Ideal Use Case | Ongoing Maintenance |\n",
    "|-----------|-----------------|----------------|---------------------|\n",
    "| Partitioning | Directory pruning | Low-cardinality, common filters | Low once chosen |\n",
    "| Z-Ordering | Multi-column data skipping | Repeated predicates across moderate/high-card columns | Periodic re-run as data grows |\n",
    "| Manual Compaction | Fewer larger files | Backfill / bursty small writes | Run as needed (monitor file size dist) |\n",
    "| Auto Optimize | Reduce small files proactively | Continuous ingestion workloads | Minimal |\n",
    "| Liquid Clustering | Adaptive layout | Dynamic predicates & evolving schemas | Managed (monitor clustering metrics) |\n",
    "| VACUUM | Storage reclamation | Any table with churn | Scheduled (respect retention) |\n",
    "\n",
    "### Key Heuristics\n",
    "- Start simple: partition only when clear benefit.\n",
    "- Use Z-Order or Liquid Clustering to refine skipping without over-partitioning.\n",
    "- Monitor file counts & average file size (DESCRIBE DETAIL) for compaction signals.\n",
    "- Keep time travel / retention policies aligned with data recovery requirements.\n",
    "\n",
    "### Suggested Next Experiments\n",
    "1. Add a date-based partition layer and compare with country partitioning.\n",
    "2. Introduce updates/deletes & measure impact on small file generation.\n",
    "3. Benchmark query patterns with and without Z-Order refresh after significant data growth.\n",
    "4. Use Photon runtime and compare scan metrics.\n",
    "5. Track metrics programmatically and plot improvements over time.\n",
    "\n",
    "## Cleanup\n",
    "Run the following cell to drop the catalog and all associated tables created during this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4afdf617-a65a-44ad-852a-0783313a446c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the catalog and all its contents\n",
    "# spark.sql(f\"DROP CATALOG IF EXISTS {CATALOG_NAME} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
