{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf1e7c9-ce3f-4ef8-ae77-9a75dd01aab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake Optimization Project\n",
    "\n",
    "This notebook is a guided, hands‑on lab to explore core Delta Lake optimization techniques in Databricks. You'll iteratively: generate data → profile baseline performance → apply optimizations → observe impact in the Spark UI and table metadata.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end you should be able to:\n",
    "- Select appropriate physical layout strategies (partitioning, Z-Ordering, Liquid Clustering)\n",
    "- Diagnose small-file and data-skipping issues\n",
    "- Apply manual vs automatic compaction\n",
    "- Use VACUUM safely for storage hygiene\n",
    "- Read/interpret Spark UI metrics (files scanned, data read, predicate pushdown, scan time)\n",
    "\n",
    "## Flow Overview\n",
    "1. Environment + table name registry\n",
    "2. Data generation (idempotent)\n",
    "3. Baseline query + metrics capture checklist\n",
    "4. Partitioning impact\n",
    "5. Z-Ordering for multi-dimensional skipping\n",
    "6. Manual vs auto compaction\n",
    "7. Liquid Clustering (adaptive layout)\n",
    "8. VACUUM lifecycle management\n",
    "9. Cleanup\n",
    "\n",
    "> Tip: After each transformation, open the Spark UI and record: files scanned, total data read MB, and wall-clock time. Building a mini table of results reinforces the concepts.\n",
    "\n",
    "## Setup\n",
    "We first define the catalog & schema (database) and then a centralized registry of logical table names to keep code DRY and readable. Ensure your workspace permissions allow catalog/schema creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55ad87c-8d1f-4793-bfd0-ec6cc7890111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG_NAME = \"delta_optimization_project\"\n",
    "SCHEMA_NAME = \"sales_data\"\n",
    "\n",
    "# Create the catalog and schema if they don't exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cdf085-bd15-463c-8400-d02724dc4fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Centralized table name registry & helper utilities\n",
    "TABLES = {\n",
    "    \"raw\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\",\n",
    "    \"partitioned\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_partitioned\",\n",
    "    \"raw_zorder\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw_zorder\",\n",
    "    \"to_compact\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_to_compact\",\n",
    "    \"auto_compact\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_auto_compact\",\n",
    "    \"liquid_clustered\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_liquid_clustered\",\n",
    "}\n",
    "\n",
    "def tbl(key: str) -> str:\n",
    "    \"\"\"Return fully qualified table name by logical key.\"\"\"\n",
    "    return TABLES[key]\n",
    "\n",
    "from typing import Optional\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fee3ecd-401f-44a9-853e-dad5239e3076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Generate Synthetic Sales Data\n",
    "\n",
    "We create a moderately large, semi-realistic sales fact dataset to exercise partition pruning and data skipping. Characteristics:\n",
    "- 5M rows (adjustable) to give enough files for optimization to matter\n",
    "- Mixed cardinalities: `country` (low), `customer_id` (medium/high), `product_id` (medium) for contrasting layout strategies\n",
    "- Temporal column `sale_date` enabling potential future partitioning or clustering experiments\n",
    "\n",
    "### Design Notes\n",
    "- Data generation is idempotent: if the base table exists we skip regeneration (saves cluster time)\n",
    "- We intentionally over-partition the initial write (`num_partitions` + low `maxRecordsPerFile`) to create many small files and highlight gains from compaction\n",
    "\n",
    "### Your Tasks\n",
    "1. Run the cell.\n",
    "2. If generation runs, note: number of partitions written, approximate file count (later visible via `DESCRIBE DETAIL` or storage listing).\n",
    "3. If skipped, confirm reuse message printed.\n",
    "\n",
    "Proceed once the table exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb951266-0107-4134-9962-a0d91ad0d06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data at scale using Spark (executor-friendly, no pandas/Faker)\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# If the target table already exists, skip expensive generation and reuse it.\n",
    "# This makes the notebook idempotent for iterative runs.\n",
    "raw_exists = spark.sql(f\"SHOW TABLES IN {CATALOG_NAME}.{SCHEMA_NAME} LIKE 'sales_raw'\").count() > 0\n",
    "if raw_exists:\n",
    "    print(f\"Table {tbl('raw')} already exists; skipping data generation.\")\n",
    "else:\n",
    "    # Number of records to generate (adjust for your cluster)\n",
    "    num_records = 5_000_000\n",
    "\n",
    "    # Controls the number of output files (one file per output partition, roughly).\n",
    "    # Increase partitions to create more small files; decrease to reduce file count.\n",
    "    num_partitions = 1000\n",
    "\n",
    "    # Small list of countries used for sampling (kept on driver only)\n",
    "    countries = ['United States','United Kingdom','Germany','France','Canada','Australia','India','Brazil','Japan','Netherlands']\n",
    "\n",
    "    # Build the DataFrame using Spark primitives (scales across executors)\n",
    "    df = (\n",
    "        spark.range(num_records)\n",
    "        .repartition(num_partitions)\n",
    "        .withColumnRenamed('id', 'seq')\n",
    "        .withColumn(\n",
    "            'transaction_id',\n",
    "            F.concat(\n",
    "                F.lit('txn_'),\n",
    "                F.col('seq').cast('string'),\n",
    "                F.lit('_'),\n",
    "                # cast the double returned by rand() to string before md5 to avoid datatype mismatch\n",
    "                F.substring(F.md5(F.rand(12345).cast('string')), 1, 8)\n",
    "            )\n",
    "        )\n",
    "        .withColumn('customer_id', (F.floor(F.rand(42) * 1001) + 1000).cast('int'))\n",
    "        .withColumn('product_id', (F.floor(F.rand(99) * 401) + 100).cast('int'))\n",
    "        .withColumn('sale_date', F.date_sub(F.current_date(), F.floor(F.rand(7) * 730).cast('int')))\n",
    "        .withColumn('quantity', (F.floor(F.rand(11) * 10) + 1).cast('int'))\n",
    "        .withColumn('unit_price', F.round(F.rand(13) * 190.0 + F.lit(10.5), 2))\n",
    "        .withColumn(\n",
    "            'country',\n",
    "            F.element_at(\n",
    "                F.array(*[F.lit(c) for c in countries]),\n",
    "                (F.floor(F.rand(21) * len(countries)) + 1).cast('int')\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Write the data to a Delta table. Use maxRecordsPerFile to force small files in a performant, distributed write.\n",
    "    (df.write\n",
    "       .format('delta')\n",
    "       .option('maxRecordsPerFile', 5000)\n",
    "       .mode('overwrite')\n",
    "       .saveAsTable(tbl('raw'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e09cc55-284c-49cf-98fb-93b2d6f86220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.table(tbl('raw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed24c04-8d39-40de-b15e-f4eb87caa882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Baseline Query (Unoptimized Table)\n",
    "\n",
    "We establish a performance baseline before altering layout. This isolates the effect of later optimizations.\n",
    "\n",
    "### Query Pattern\n",
    "Filter on a specific `(customer_id, product_id)` pair. This mimics a targeted lookup often seen in downstream analytics or service patterns.\n",
    "\n",
    "### What to Capture (Create a simple log table for yourself)\n",
    "| Metric | Value | How to Find |\n",
    "|--------|-------|-------------|\n",
    "| Files scanned | ? | Spark UI -> SQL/Dataframe -> Scan node |\n",
    "| Bytes read | ? | Spark UI -> Scan details |\n",
    "| Duration (s) | ? | Job / Stage timeline |\n",
    "| Output rows | ? | DataFrame action output |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656aaf49-e263-4406-aeee-ddd5173eb8fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7f6562-c39b-4656-9d0a-83d0a021d677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Baseline query — read from saved Delta table\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_raw.where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3954ff1d-56fb-4a51-998b-91f77ec31743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Open Spark UI while it is/after it finishes.\n",
    "3. Record metrics above.\n",
    "4. Keep this snapshot; you'll compare after each optimization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70376fa4-8432-46c6-9482-3fe1977bd3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Implementing Partitioning\n",
    "\n",
    "Partitioning physically groups data by directory on chosen columns. Pros: efficient directory pruning when filters match partition predicates. Cons: too many partitions => small files & metadata overhead; high-cardinality partition keys are an anti-pattern.\n",
    "\n",
    "We choose `country` (low cardinality ~10) as a demonstrative partition column. Even though our query predicate currently does *not* include `country`, observing the plan difference illustrates when partitioning helps or is neutral.\n",
    "\n",
    "### Good Candidates\n",
    "- Low to moderate cardinality (10s–100s distinct) used frequently in filters\n",
    "- Dimension-style attributes (e.g., region, date (at coarse grain), category)\n",
    "\n",
    "### Avoid\n",
    "- High-cardinality (user ids, transaction ids)\n",
    "- Rapidly growing distinct sets (timestamps to the second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61b8f31-ea60-4ae1-ad0f-330e0a384c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cda5f2b-d52e-4f81-ab6b-a03a0ba4888e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df_raw.write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"country\")\n",
    " .saveAsTable(tbl('partitioned')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6babd604-1382-4007-b535-a77d27f3abc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the same query on the partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f8a113-f8fe-42ce-a3f5-0c0e731a7643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.read.table(tbl('partitioned'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07946c7a-6a57-430e-957d-10b77e25dab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Partitioning:**\n",
    "\n",
    "Go to the Spark UI and compare the execution plan with the baseline. You'll notice that if a filter on the partition key (`country`) was present, Spark would be able to prune entire directories, significantly reducing the amount of data scanned. Even without a direct filter on the partition key, observe any changes in the query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6ddb28-5570-4ddf-8c24-3e3d1f7c1b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Applying Z-Ordering\n",
    "\n",
    "Z-Ordering re-writes data files using a multi-dimensional space-filling curve (Z-order) so rows with similar values for selected columns co-locate. This increases data skipping efficiency without exploding directory counts like over-partitioning.\n",
    "\n",
    "### When to Use\n",
    "- Repeated predicates on 1–4 moderate/high cardinality columns\n",
    "- Mixed access patterns (can't commit to a single partition key)\n",
    "- Need to cluster on columns that would be poor partitions\n",
    "\n",
    "### Column Choice Tips\n",
    "| Scenario | Better for Partition | Better for Z-Order |\n",
    "|----------|---------------------|--------------------|\n",
    "| Very low cardinality | Yes | Unnecessary |\n",
    "| High cardinality | No | Often yes |\n",
    "| Evolving query set | Maybe (limited) | Yes |\n",
    "\n",
    "We Z-Order on `customer_id`, `product_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5395cd-c20c-4463-bfa5-e5abb40c0eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create Z-Ordered copy of raw table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e2c337-e291-4f2c-bd22-1b303a0ce6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {tbl('raw_zorder')}\")\n",
    "\n",
    "# Create a separate table from sales_raw to apply Z-Ordering to (keeps sales_partitioned untouched)\n",
    "spark.sql(f\"CREATE TABLE {tbl('raw_zorder')} USING DELTA AS SELECT * FROM {tbl('raw')}\")\n",
    "\n",
    "# Convert the new table to a DeltaTable object\n",
    "delta_table = DeltaTable.forName(spark, tbl('raw_zorder'))\n",
    "\n",
    "# Apply Z-Ordering on high-cardinality columns for the raw copy\n",
    "delta_table.optimize().executeZOrderBy(\"customer_id\", \"product_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e85686-d7b8-4950-885f-2463b4176d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rerun the query to see the effect of Z-Ordering on the z-ordered raw table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71081abd-5829-44f1-9615-f1ae6504645a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.read.table(tbl('raw_zorder'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f8f068-2eb1-4e0c-accd-088aa03d8e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Z-Ordering:**\n",
    "\n",
    "In the Spark UI, look at the \"Details\" for the scan phase. You should see a significant reduction in the number of files read, demonstrating the effectiveness of data skipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12f0bfd-6b53-4cc6-ac13-628fcec8d41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Manual Compaction (OPTIMIZE)\n",
    "\n",
    "Small files inflate job overhead (task scheduling, metadata reads, open/close costs) and degrade predicate pushdown (more file headers to scan). `OPTIMIZE` rewrites many small files into fewer larger ones (typically 256MB target on Databricks unless overridden).\n",
    "\n",
    "### Simulation\n",
    "We append tiny random samples repeatedly to manufacture fragmentation.\n",
    "\n",
    "> Rule of thumb: Target 100–500MB per file for large analytical tables (depends on workload)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1ae396-dd98-4970-a8b6-837e08cacc00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run loop to append small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f46e46-c051-492b-b764-aa85e460d615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate small file creation by writing in a loop\n",
    "for i in range(10):\n",
    "    (df_raw.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(tbl('to_compact')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c694f9-9bf1-43f1-9a55-c2fd6a4ce43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "List number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e274cbb-ad3c-4acb-9448-e70568526d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of files\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('to_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b419f69-b7ba-48f4-b0d4-4a33515a0dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Execute `OPTIMIZE` (compaction only — no Z-order here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7f9cb9-d327-4f53-b2d0-c5babe1d6594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now, perform manual compaction\n",
    "delta_table_to_compact = DeltaTable.forName(spark, tbl('to_compact'))\n",
    "delta_table_to_compact.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36711163-356e-4526-b8f5-f39b0617bfb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Re-list object count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46ad8d3-af86-461c-985b-5f7f8440f4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of files\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('to_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d31b79-8287-4595-b596-505b4addbbda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Compaction:**\n",
    "\n",
    "- Observe the reduction in the number of files after running `OPTIMIZE`. This leads to more efficient reads as Spark needs to open and process fewer files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caea28a7-a588-4049-9bf3-2cec0b8040ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Auto Compaction\n",
    "\n",
    "Auto Optimize (optimizeWrites + autoCompact) reduces operational toil. It sizes files during write and schedules asynchronous compaction of lingering small files.\n",
    "\n",
    "### What It Does\n",
    "- optimizeWrite: coalesces output at write time to hit target size\n",
    "- autoCompact: background job merges residual small files post-commit\n",
    "\n",
    "### Caveats\n",
    "- Not a substitute for strategic Z-Ordering or Liquid Clustering\n",
    "- Background lag: immediate file count after write may still be high briefly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3020ea28-6842-4054-bdcd-df5bf9e069a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create table with properties enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0df6bd1-eb9e-413a-aa8e-655ef87009b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a table with auto-compaction enabled\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {tbl('auto_compact')}\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "    'delta.autoOptimize.autoCompact' = 'true'\n",
    ")\n",
    "AS SELECT * FROM {tbl('raw')}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c7275c-e79b-4225-8168-869abd37d0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Perform small appends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0fdcb1b-e213-4d94-b07b-500615f20d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Simulate small writes to this table\n",
    "for i in range(10):\n",
    "    (df_raw.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(tbl('auto_compact')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e597a7-9c34-48a6-8ccf-f609c86cb45c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Auto Compaction:**\n",
    "\n",
    "- After the writes complete, query the table's history to see the `OPTIMIZE` operations that were automatically triggered. Auto-compaction helps maintain optimal file sizes without manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ec5d0d-9f9d-4bea-b33a-66892e0bbd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"DESCRIBE HISTORY {tbl('auto_compact')}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dce0814-4dd3-4592-ad37-be363b962a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Liquid Clustering\n",
    "\n",
    "Liquid Clustering decouples logical clustering from static partitions. Databricks can dynamically maintain clustering as data evolves without rigid directory hierarchies.\n",
    "\n",
    "### Advantages vs Partitioning/Z-Order\n",
    "- Adaptive: automatically reorganizes incremental data\n",
    "- Multi-column \"soft\" clustering without explosion in partitions\n",
    "- Plays well with schema evolution and changing query shapes\n",
    "\n",
    "### When to Prefer\n",
    "- High churn / streaming upserts\n",
    "- Many predicates across overlapping column sets\n",
    "- Avoiding maintenance burden of periodic manual Z-Order runs\n",
    "\n",
    "> Consider long-term ops: Which technique minimizes recurring maintenance for your workload?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01912d9-5503-4777-b2d3-67b72c76a011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create clustered table and load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced3e22f-f5d6-4469-9d67-1c64a410c9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {tbl('liquid_clustered')} (\n",
    "  seq LONG,\n",
    "  transaction_id STRING,\n",
    "  customer_id INT,\n",
    "  product_id INT,\n",
    "  sale_date DATE,\n",
    "  quantity INT,\n",
    "  unit_price DOUBLE,\n",
    "  country STRING\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY (customer_id, product_id)\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into the clustered table\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(tbl('liquid_clustered'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf1f615-a4d7-4819-99b2-453af4e8dab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run predicate query; record scan metrics. Compare with Z-Order table results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1b139c-240d-478b-90bd-2f4cb592e840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rerun the query on the liquid clustered table\n",
    "(spark.read.table(tbl('liquid_clustered'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40780f01-0b38-43ff-8121-c122be60fc22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Liquid Clustering:**\n",
    "\n",
    "1.  Run the query on the liquid clustered table.\n",
    "2.  Examine the Spark UI. Liquid Clustering provides similar data skipping benefits as Z-Ordering but is more adaptive to changes in your data and queries over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c685441-e670-42b6-9f94-bc5df71751f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: VACUUM\n",
    "\n",
    "`VACUUM` removes obsolete files no longer referenced by the Delta transaction log (old snapshots, overwritten data). This reclaims storage and can reduce metadata overhead.\n",
    "\n",
    "### Safety Mechanism\n",
    "Default retention (168h / 7d) protects against readers holding older snapshots (time travel) and against eventual consistency edge cases.\n",
    "\n",
    "### Demo Adjustment\n",
    "We disable retention checks only for demonstration to show effect immediately — *never do this in production* unless you fully understand snapshot isolation & streaming lag impacts.\n",
    "\n",
    "> Production Practice: Keep retention ≥ 7 days; align with compliance & restore objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0484beba-c8d9-4fff-961e-54842199f71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform updates/deletes before VACUUM to create orphaned files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b12896-a3ce-4358-be4c-bb774bb20e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example update: Increase quantity by 10 for a specific customer\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {tbl('partitioned')}\n",
    "SET quantity = quantity + 10\n",
    "WHERE customer_id >= 1500\n",
    "\"\"\")\n",
    "\n",
    "# Example delete: Remove records for a specific product\n",
    "spark.sql(f\"\"\"\n",
    "DELETE FROM {tbl('partitioned')}\n",
    "WHERE product_id <= 250\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b21fea-26e5-41a9-b357-621164bafdb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check number of files before vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef2f1ce-2402-4528-977c-cd3dd8619e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('partitioned')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5fcba2-00ff-4c5b-aff3-90209bdb89a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run VACUUM\n",
    "\n",
    "> **Note:**  \n",
    "> In Databricks Free Edition, you cannot disable the `retentionDurationCheck` safety feature.  \n",
    "> This means you may need to wait up to **7 days** after performing update or delete operations before orphaned files can be removed by VACUUM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9932fb18-04f7-4eab-ae69-f3ca0a4baf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable the retention check\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "\n",
    "# VACUUM the partitioned table\n",
    "delta_table_to_vacuum = DeltaTable.forName(spark, tbl('partitioned'))\n",
    "delta_table_to_vacuum.vacuum()\n",
    "\n",
    "# Re-enable the retention check\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1e0ad1-f23b-4f02-ae9f-6d4d32cc593a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check number of files after vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6a721d-6bbd-4bbc-a0f0-796953064b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE DETAIL {tbl('partitioned')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eb9dbd-3bdd-4d77-b916-90c5e4a76468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of VACUUM:**\n",
    "\n",
    "- The `VACUUM` command cleans up files that are no longer part of the current version of the table. You can check the file system before and after running `VACUUM` on a table that has undergone several modifications to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a2d933-6c9b-4eee-b651-081c3d9771cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Conclusion\n",
    "\n",
    "You applied multiple, complementary Delta optimization techniques and observed their impact empirically.\n",
    "\n",
    "### Comparative Summary (Qualitative)\n",
    "| Technique | Primary Benefit | Ideal Use Case | Ongoing Maintenance |\n",
    "|-----------|-----------------|----------------|---------------------|\n",
    "| Partitioning | Directory pruning | Low-cardinality, common filters | Low once chosen |\n",
    "| Z-Ordering | Multi-column data skipping | Repeated predicates across moderate/high-card columns | Periodic re-run as data grows |\n",
    "| Manual Compaction | Fewer larger files | Backfill / bursty small writes | Run as needed (monitor file size dist) |\n",
    "| Auto Optimize | Reduce small files proactively | Continuous ingestion workloads | Minimal |\n",
    "| Liquid Clustering | Adaptive layout | Dynamic predicates & evolving schemas | Managed (monitor clustering metrics) |\n",
    "| VACUUM | Storage reclamation | Any table with churn | Scheduled (respect retention) |\n",
    "\n",
    "### Key Heuristics\n",
    "- Start simple: partition only when clear benefit.\n",
    "- Use Z-Order or Liquid Clustering to refine skipping without over-partitioning.\n",
    "- Monitor file counts & average file size (DESCRIBE DETAIL) for compaction signals.\n",
    "- Keep time travel / retention policies aligned with data recovery requirements.\n",
    "\n",
    "### Suggested Next Experiments\n",
    "1. Add a date-based partition layer and compare with country partitioning.\n",
    "2. Introduce updates/deletes & measure impact on small file generation.\n",
    "3. Benchmark query patterns with and without Z-Order refresh after significant data growth.\n",
    "4. Use Photon runtime and compare scan metrics.\n",
    "5. Track metrics programmatically and plot improvements over time.\n",
    "\n",
    "## Cleanup\n",
    "Run the following cell to drop the catalog and all associated tables created during this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4afdf617-a65a-44ad-852a-0783313a446c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the catalog and all its contents\n",
    "# spark.sql(f\"DROP CATALOG IF EXISTS {CATALOG_NAME} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
