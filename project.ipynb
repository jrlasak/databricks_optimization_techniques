{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb54111",
   "metadata": {},
   "source": [
    "# Delta Lake Optimization Project\n",
    "\n",
    "This notebook is a guided, hands‑on lab to explore core Delta Lake optimization techniques in Databricks. You'll iteratively: generate data → profile baseline performance → apply optimizations → observe impact in the Spark UI and table metadata.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end you should be able to:\n",
    "- Select appropriate physical layout strategies (partitioning, Z-Ordering, Liquid Clustering)\n",
    "- Diagnose small-file and data-skipping issues\n",
    "- Apply manual vs automatic compaction\n",
    "- Use VACUUM safely for storage hygiene\n",
    "- Read/interpret Spark UI metrics (files scanned, data read, predicate pushdown, scan time)\n",
    "\n",
    "## Flow Overview\n",
    "1. Environment + table name registry\n",
    "2. Data generation (idempotent)\n",
    "3. Baseline query + metrics capture checklist\n",
    "4. Partitioning impact\n",
    "5. Z-Ordering for multi-dimensional skipping\n",
    "6. Manual vs auto compaction\n",
    "7. Liquid Clustering (adaptive layout)\n",
    "8. VACUUM lifecycle management\n",
    "9. Cleanup\n",
    "\n",
    "> Tip: After each transformation, open the Spark UI and record: files scanned, total data read MB, and wall-clock time. Building a mini table of results reinforces the concepts.\n",
    "\n",
    "## Setup\n",
    "We first define the catalog & schema (database) and then a centralized registry of logical table names to keep code DRY and readable. Ensure your workspace permissions allow catalog/schema creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG_NAME = \"delta_optimization_project\"\n",
    "SCHEMA_NAME = \"sales_data\"\n",
    "\n",
    "# Create the catalog and schema if they don't exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized table name registry & helper utilities\n",
    "TABLES = {\n",
    "    \"raw\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\",\n",
    "    \"partitioned\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_partitioned\",\n",
    "    \"raw_zorder\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw_zorder\",\n",
    "    \"to_compact\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_to_compact\",\n",
    "    \"auto_compact\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_auto_compact\",\n",
    "    \"liquid_clustered\": f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_liquid_clustered\",\n",
    "}\n",
    "\n",
    "def tbl(key: str) -> str:\n",
    "    \"\"\"Return fully qualified table name by logical key.\"\"\"\n",
    "    return TABLES[key]\n",
    "\n",
    "from typing import Optional\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818a1c6",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Sales Data\n",
    "\n",
    "We create a moderately large, semi-realistic sales fact dataset to exercise partition pruning and data skipping. Characteristics:\n",
    "- 5M rows (adjustable) to give enough files for optimization to matter\n",
    "- Mixed cardinalities: `country` (low), `customer_id` (medium/high), `product_id` (medium) for contrasting layout strategies\n",
    "- Temporal column `sale_date` enabling potential future partitioning or clustering experiments\n",
    "\n",
    "### Design Notes\n",
    "- Data generation is idempotent: if the base table exists we skip regeneration (saves cluster time)\n",
    "- We intentionally over-partition the initial write (`num_partitions` + low `maxRecordsPerFile`) to create many small files and highlight gains from compaction\n",
    "\n",
    "### Your Tasks\n",
    "1. Run the cell.\n",
    "2. If generation runs, note: number of partitions written, approximate file count (later visible via `DESCRIBE DETAIL` or storage listing).\n",
    "3. If skipped, confirm reuse message printed.\n",
    "\n",
    "Proceed once the table exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data at scale using Spark (executor-friendly, no pandas/Faker)\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# If the target table already exists, skip expensive generation and reuse it.\n",
    "# This makes the notebook idempotent for iterative runs.\n",
    "raw_exists = spark.sql(f\"SHOW TABLES IN {CATALOG_NAME}.{SCHEMA_NAME} LIKE 'sales_raw'\").count() > 0\n",
    "if raw_exists:\n",
    "    print(f\"Table {tbl('raw')} already exists; skipping data generation.\")\n",
    "else:\n",
    "    # Number of records to generate (adjust for your cluster)\n",
    "    num_records = 5_000_000\n",
    "\n",
    "    # Controls the number of output files (one file per output partition, roughly).\n",
    "    # Increase partitions to create more small files; decrease to reduce file count.\n",
    "    num_partitions = 1000\n",
    "\n",
    "    # Small list of countries used for sampling (kept on driver only)\n",
    "    countries = ['United States','United Kingdom','Germany','France','Canada','Australia','India','Brazil','Japan','Netherlands']\n",
    "\n",
    "    # Build the DataFrame using Spark primitives (scales across executors)\n",
    "    df = (\n",
    "        spark.range(num_records)\n",
    "        .repartition(num_partitions)\n",
    "        .withColumnRenamed('id', 'seq')\n",
    "        .withColumn(\n",
    "            'transaction_id',\n",
    "            F.concat(\n",
    "                F.lit('txn_'),\n",
    "                F.col('seq').cast('string'),\n",
    "                F.lit('_'),\n",
    "                # cast the double returned by rand() to string before md5 to avoid datatype mismatch\n",
    "                F.substring(F.md5(F.rand(12345).cast('string')), 1, 8)\n",
    "            )\n",
    "        )\n",
    "        .withColumn('customer_id', (F.floor(F.rand(42) * 1001) + 1000).cast('int'))\n",
    "        .withColumn('product_id', (F.floor(F.rand(99) * 401) + 100).cast('int'))\n",
    "        .withColumn('sale_date', F.date_sub(F.current_date(), F.floor(F.rand(7) * 730).cast('int')))\n",
    "        .withColumn('quantity', (F.floor(F.rand(11) * 10) + 1).cast('int'))\n",
    "        .withColumn('unit_price', F.round(F.rand(13) * 190.0 + F.lit(10.5), 2))\n",
    "        .withColumn(\n",
    "            'country',\n",
    "            F.element_at(\n",
    "                F.array(*[F.lit(c) for c in countries]),\n",
    "                (F.floor(F.rand(21) * len(countries)) + 1).cast('int')\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Write the data to a Delta table. Use maxRecordsPerFile to force small files in a performant, distributed write.\n",
    "    (df.write\n",
    "       .format('delta')\n",
    "       .option('maxRecordsPerFile', 5000)\n",
    "       .mode('overwrite')\n",
    "       .saveAsTable(tbl('raw'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02911ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.table(tbl('raw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d004ae",
   "metadata": {},
   "source": [
    "## Step 2: Baseline Query (Unoptimized Table)\n",
    "\n",
    "We establish a performance baseline before altering layout. This isolates the effect of later optimizations.\n",
    "\n",
    "### Query Pattern\n",
    "Filter on a specific `(customer_id, product_id)` pair. This mimics a targeted lookup often seen in downstream analytics or service patterns.\n",
    "\n",
    "### What to Capture (Create a simple log table for yourself)\n",
    "| Metric | Value | How to Find |\n",
    "|--------|-------|-------------|\n",
    "| Files scanned | ? | Spark UI -> SQL/Dataframe -> Scan node |\n",
    "| Bytes read | ? | Spark UI -> Scan details |\n",
    "| Duration (s) | ? | Job / Stage timeline |\n",
    "| Output rows | ? | DataFrame action output |\n",
    "\n",
    "### Tasks\n",
    "1. Run the cell below.\n",
    "2. Open Spark UI while it is/after it finishes.\n",
    "3. Record metrics above.\n",
    "4. Keep this snapshot; you'll compare after each optimization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline query — read from saved Delta table\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_raw.where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57316fef",
   "metadata": {},
   "source": [
    "## Step 3: Implementing Partitioning\n",
    "\n",
    "Partitioning physically groups data by directory on chosen columns. Pros: efficient directory pruning when filters match partition predicates. Cons: too many partitions => small files & metadata overhead; high-cardinality partition keys are an anti-pattern.\n",
    "\n",
    "We choose `country` (low cardinality ~10) as a demonstrative partition column. Even though our query predicate currently does *not* include `country`, observing the plan difference illustrates when partitioning helps or is neutral.\n",
    "\n",
    "### Good Candidates\n",
    "- Low to moderate cardinality (10s–100s distinct) used frequently in filters\n",
    "- Dimension-style attributes (e.g., region, date (at coarse grain), category)\n",
    "\n",
    "### Avoid\n",
    "- High-cardinality (user ids, transaction ids)\n",
    "- Rapidly growing distinct sets (timestamps to the second)\n",
    "\n",
    "### Tasks\n",
    "1. Create partitioned table.\n",
    "2. Re-run the same predicate query.\n",
    "3. Compare metrics table vs baseline (expect similar unless you add a country filter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f70e16",
   "metadata": {},
   "source": [
    "Create a partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2249ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_raw.write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"country\")\n",
    " .saveAsTable(tbl('partitioned')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658e377",
   "metadata": {},
   "source": [
    "Run the same query on the partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfccd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read.table(tbl('partitioned'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce1725",
   "metadata": {},
   "source": [
    "**Analysis of Partitioning:**\n",
    "\n",
    "Go to the Spark UI and compare the execution plan with the baseline. You'll notice that if a filter on the partition key (`country`) was present, Spark would be able to prune entire directories, significantly reducing the amount of data scanned. Even without a direct filter on the partition key, observe any changes in the query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8bf88",
   "metadata": {},
   "source": [
    "## Step 4: Applying Z-Ordering\n",
    "\n",
    "Z-Ordering re-writes data files using a multi-dimensional space-filling curve (Z-order) so rows with similar values for selected columns co-locate. This increases data skipping efficiency without exploding directory counts like over-partitioning.\n",
    "\n",
    "### When to Use\n",
    "- Repeated predicates on 1–4 moderate/high cardinality columns\n",
    "- Mixed access patterns (can't commit to a single partition key)\n",
    "- Need to cluster on columns that would be poor partitions\n",
    "\n",
    "### Column Choice Tips\n",
    "| Scenario | Better for Partition | Better for Z-Order |\n",
    "|----------|---------------------|--------------------|\n",
    "| Very low cardinality | Yes | Unnecessary |\n",
    "| High cardinality | No | Often yes |\n",
    "| Evolving query set | Maybe (limited) | Yes |\n",
    "\n",
    "We Z-Order on `customer_id`, `product_id`.\n",
    "\n",
    "### Tasks\n",
    "1. Create Z-Ordered copy of raw table.\n",
    "2. Re-run predicate query.\n",
    "3. Record files scanned & bytes read — should drop notably vs baseline.\n",
    "4. Reflect: Would partitioning on these columns have been feasible? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24eea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {tbl('raw_zorder')}\")\n",
    "\n",
    "# Create a separate table from sales_raw to apply Z-Ordering to (keeps sales_partitioned untouched)\n",
    "spark.sql(f\"CREATE TABLE {tbl('raw_zorder')} USING DELTA AS SELECT * FROM {tbl('raw')}\")\n",
    "\n",
    "# Convert the new table to a DeltaTable object\n",
    "delta_table = DeltaTable.forName(spark, tbl('raw_zorder'))\n",
    "\n",
    "# Apply Z-Ordering on high-cardinality columns for the raw copy\n",
    "delta_table.optimize().executeZOrderBy(\"customer_id\", \"product_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe44c2",
   "metadata": {},
   "source": [
    "Rerun the query to see the effect of Z-Ordering on the z-ordered raw table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read.table(tbl('raw_zorder'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34832b",
   "metadata": {},
   "source": [
    "**Analysis of Z-Ordering:**\n",
    "\n",
    "In the Spark UI, look at the \"Details\" for the scan phase. You should see a significant reduction in the number of files read, demonstrating the effectiveness of data skipping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace5215",
   "metadata": {},
   "source": [
    "## Step 5: Manual Compaction (OPTIMIZE)\n",
    "\n",
    "Small files inflate job overhead (task scheduling, metadata reads, open/close costs) and degrade predicate pushdown (more file headers to scan). `OPTIMIZE` rewrites many small files into fewer larger ones (typically 256MB target on Databricks unless overridden).\n",
    "\n",
    "### Simulation\n",
    "We append tiny random samples repeatedly to manufacture fragmentation.\n",
    "\n",
    "### Tasks\n",
    "1. Run loop to append small data.\n",
    "2. List storage to observe many small part files.\n",
    "3. Execute `OPTIMIZE` (compaction only — no Z-order here).\n",
    "4. Re-list storage; note file count reduction & larger sizes.\n",
    "5. Optionally re-run baseline query and record scan metrics.\n",
    "\n",
    "> Rule of thumb: Target 100–500MB per file for large analytical tables (depends on workload)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e668fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate small file creation by writing in a loop\n",
    "for i in range(10):\n",
    "    (df_raw.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(tbl('to_compact')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of files\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('to_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, perform manual compaction\n",
    "delta_table_to_compact = DeltaTable.forName(spark, tbl('to_compact'))\n",
    "delta_table_to_compact.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1755b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of files\n",
    "spark.sql(f\"DESCRIBE DETAIL {tbl('to_compact')}\").select(\"numFiles\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c727a",
   "metadata": {},
   "source": [
    "**Analysis of Compaction:**\n",
    "\n",
    "- Observe the reduction in the number of files after running `OPTIMIZE`. This leads to more efficient reads as Spark needs to open and process fewer files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6967f",
   "metadata": {},
   "source": [
    "## Step 6: Auto Compaction\n",
    "\n",
    "Auto Optimize (optimizeWrites + autoCompact) reduces operational toil. It sizes files during write and schedules asynchronous compaction of lingering small files.\n",
    "\n",
    "### What It Does\n",
    "- optimizeWrite: coalesces output at write time to hit target size\n",
    "- autoCompact: background job merges residual small files post-commit\n",
    "\n",
    "### Caveats\n",
    "- Not a substitute for strategic Z-Ordering or Liquid Clustering\n",
    "- Background lag: immediate file count after write may still be high briefly\n",
    "\n",
    "### Tasks\n",
    "1. Create table with properties enabled.\n",
    "2. Perform small appends.\n",
    "3. Inspect history for `OPTIMIZE` operations triggered automatically.\n",
    "4. Optionally compare average file size vs manually compacted table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af680a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with auto-compaction enabled\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {tbl('auto_compact')}\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "    'delta.autoOptimize.autoCompact' = 'true'\n",
    ")\n",
    "AS SELECT * FROM {tbl('raw')}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02552cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulate small writes to this table\n",
    "for i in range(10):\n",
    "    (df_raw.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(tbl('auto_compact')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc02aa9",
   "metadata": {},
   "source": [
    "**Analysis of Auto Compaction:**\n",
    "\n",
    "- After the writes complete, query the table's history to see the `OPTIMIZE` operations that were automatically triggered. Auto-compaction helps maintain optimal file sizes without manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805fa9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"DESCRIBE HISTORY {tbl('auto_compact')}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05036473",
   "metadata": {},
   "source": [
    "## Step 7: Liquid Clustering\n",
    "\n",
    "Liquid Clustering decouples logical clustering from static partitions. Databricks can dynamically maintain clustering as data evolves without rigid directory hierarchies.\n",
    "\n",
    "### Advantages vs Partitioning/Z-Order\n",
    "- Adaptive: automatically reorganizes incremental data\n",
    "- Multi-column \"soft\" clustering without explosion in partitions\n",
    "- Plays well with schema evolution and changing query shapes\n",
    "\n",
    "### When to Prefer\n",
    "- High churn / streaming upserts\n",
    "- Many predicates across overlapping column sets\n",
    "- Avoiding maintenance burden of periodic manual Z-Order runs\n",
    "\n",
    "### Tasks\n",
    "1. Create clustered table.\n",
    "2. Load data.\n",
    "3. Run predicate query; record scan metrics.\n",
    "4. Compare with Z-Order table results.\n",
    "\n",
    "> Consider long-term ops: Which technique minimizes recurring maintenance for your workload?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09965269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with Liquid Clustering\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {tbl('liquid_clustered')} (\n",
    "  transaction_id STRING,\n",
    "  customer_id INT,\n",
    "  product_id INT,\n",
    "  sale_date DATE,\n",
    "  quantity INT,\n",
    "  unit_price DOUBLE,\n",
    "  country STRING\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY (customer_id, product_id)\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into the clustered table\n",
    "df_raw.write.format(\"delta\").mode(\"append\").saveAsTable(tbl('liquid_clustered'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the query on the liquid clustered table\n",
    "(spark.read.table(tbl('liquid_clustered'))\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8426655",
   "metadata": {},
   "source": [
    "**Analysis of Liquid Clustering:**\n",
    "\n",
    "1.  Run the query on the liquid clustered table.\n",
    "2.  Examine the Spark UI. Liquid Clustering provides similar data skipping benefits as Z-Ordering but is more adaptive to changes in your data and queries over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060e6c8",
   "metadata": {},
   "source": [
    "## Step 8: VACUUM\n",
    "\n",
    "`VACUUM` removes obsolete files no longer referenced by the Delta transaction log (old snapshots, overwritten data). This reclaims storage and can reduce metadata overhead.\n",
    "\n",
    "### Safety Mechanism\n",
    "Default retention (168h / 7d) protects against readers holding older snapshots (time travel) and against eventual consistency edge cases.\n",
    "\n",
    "### Demo Adjustment\n",
    "We disable retention checks only for demonstration to show effect immediately — *never do this in production* unless you fully understand snapshot isolation & streaming lag impacts.\n",
    "\n",
    "### Tasks\n",
    "1. (Optional) Perform updates/deletes before VACUUM to create orphaned files.\n",
    "2. Run VACUUM.\n",
    "3. Inspect file listing before/after.\n",
    "4. Re-enable safety setting.\n",
    "\n",
    "> Production Practice: Keep retention ≥ 7 days; align with compliance & restore objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ec0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the retention period check for demonstration purposes\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# VACUUM the partitioned table\n",
    "delta_table_to_vacuum = DeltaTable.forName(spark, tbl('partitioned'))\n",
    "delta_table_to_vacuum.vacuum()\n",
    "\n",
    "# Re-enable the retention check\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd14ef4",
   "metadata": {},
   "source": [
    "**Analysis of VACUUM:**\n",
    "\n",
    "- The `VACUUM` command cleans up files that are no longer part of the current version of the table. You can check the file system before and after running `VACUUM` on a table that has undergone several modifications to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204a988",
   "metadata": {},
   "source": [
    "## Project Conclusion\n",
    "\n",
    "You applied multiple, complementary Delta optimization techniques and observed their impact empirically.\n",
    "\n",
    "### Comparative Summary (Qualitative)\n",
    "| Technique | Primary Benefit | Ideal Use Case | Ongoing Maintenance |\n",
    "|-----------|-----------------|----------------|---------------------|\n",
    "| Partitioning | Directory pruning | Low-cardinality, common filters | Low once chosen |\n",
    "| Z-Ordering | Multi-column data skipping | Repeated predicates across moderate/high-card columns | Periodic re-run as data grows |\n",
    "| Manual Compaction | Fewer larger files | Backfill / bursty small writes | Run as needed (monitor file size dist) |\n",
    "| Auto Optimize | Reduce small files proactively | Continuous ingestion workloads | Minimal |\n",
    "| Liquid Clustering | Adaptive layout | Dynamic predicates & evolving schemas | Managed (monitor clustering metrics) |\n",
    "| VACUUM | Storage reclamation | Any table with churn | Scheduled (respect retention) |\n",
    "\n",
    "### Key Heuristics\n",
    "- Start simple: partition only when clear benefit.\n",
    "- Use Z-Order or Liquid Clustering to refine skipping without over-partitioning.\n",
    "- Monitor file counts & average file size (DESCRIBE DETAIL) for compaction signals.\n",
    "- Keep time travel / retention policies aligned with data recovery requirements.\n",
    "\n",
    "### Suggested Next Experiments\n",
    "1. Add a date-based partition layer and compare with country partitioning.\n",
    "2. Introduce updates/deletes & measure impact on small file generation.\n",
    "3. Benchmark query patterns with and without Z-Order refresh after significant data growth.\n",
    "4. Use Photon runtime and compare scan metrics.\n",
    "5. Track metrics programmatically and plot improvements over time.\n",
    "\n",
    "## Cleanup\n",
    "Run the following cell to drop the catalog and all associated tables created during this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the catalog and all its contents\n",
    "spark.sql(f\"DROP CATALOG IF EXISTS {CATALOG_NAME} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
