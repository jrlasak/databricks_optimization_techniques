{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb54111",
   "metadata": {},
   "source": [
    "# Delta Lake Optimization Project\n",
    "\n",
    "This notebook provides a hands-on guide to optimizing Delta Lake tables in Databricks. We will explore and compare various optimization techniques using synthetically generated sales data.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "- Understand and implement table partitioning.\n",
    "- Apply Z-Ordering to improve data skipping.\n",
    "- Perform manual and automatic compaction to optimize file sizes.\n",
    "- Utilize Liquid Clustering for flexible data layout.\n",
    "- Use the VACUUM command to manage table storage.\n",
    "- Analyze Spark UI to observe execution plans and performance improvements.\n",
    "\n",
    "**Setup:**\n",
    "First, we'll define the catalog and schema for our project. Make sure you have the necessary permissions to create catalogs and schemas in your Databricks workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG_NAME = \"delta_optimization_project\"\n",
    "SCHEMA_NAME = \"sales_data\"\n",
    "\n",
    "# Create the catalog and schema if they don't exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818a1c6",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Sales Data\n",
    "\n",
    "We'll start by creating a synthetic dataset of sales transactions. This data will be the foundation for our optimization experiments. We'll include a variety of data types and cardinalities to make our tests realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data at scale using Spark (executor-friendly, no pandas/Faker)\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Number of records to generate (adjust for your cluster)\n",
    "num_records = 5_000_000\n",
    "\n",
    "# Controls the number of output files (one file per output partition, roughly).\n",
    "# Increase partitions to create more small files; decrease to reduce file count.\n",
    "num_partitions = 1000\n",
    "\n",
    "# Small list of countries used for sampling (kept on driver only)\n",
    "countries = ['United States','United Kingdom','Germany','France','Canada','Australia','India','Brazil','Japan','Netherlands']\n",
    "\n",
    "# Build the DataFrame using Spark primitives (scales across executors)\n",
    "df = (\n",
    "    spark.range(num_records)\n",
    "    .repartition(num_partitions)\n",
    "    .withColumnRenamed('id', 'seq')\n",
    "    .withColumn(\n",
    "        'transaction_id',\n",
    "        F.concat(\n",
    "            F.lit('txn_'),\n",
    "            F.col('seq').cast('string'),\n",
    "            F.lit('_'),\n",
    "            # cast the double returned by rand() to string before md5 to avoid datatype mismatch\n",
    "            F.substring(F.md5(F.rand(12345).cast('string')), 1, 8)\n",
    "        )\n",
    "    )\n",
    "    .withColumn('customer_id', (F.floor(F.rand(42) * 1001) + 1000).cast('int'))\n",
    "    .withColumn('product_id', (F.floor(F.rand(99) * 401) + 100).cast('int'))\n",
    "    .withColumn('sale_date', F.date_sub(F.current_date(), F.floor(F.rand(7) * 730).cast('int')))\n",
    "    .withColumn('quantity', (F.floor(F.rand(11) * 10) + 1).cast('int'))\n",
    "    .withColumn('unit_price', F.round(F.rand(13) * 190.0 + F.lit(10.5), 2))\n",
    "    .withColumn(\n",
    "        'country',\n",
    "        F.element_at(\n",
    "            F.array(*[F.lit(c) for c in countries]),\n",
    "            (F.floor(F.rand(21) * len(countries)) + 1).cast('int')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# Write the data to a Delta table. Use maxRecordsPerFile to force small files in a performant, distributed write.\n",
    "# Tune num_partitions and maxRecordsPerFile to get desired file sizes/counts for your cluster.\n",
    "(df.write\n",
    "   .format('delta')\n",
    "   .option('maxRecordsPerFile', 5000)\n",
    "   .mode('overwrite')\n",
    "   .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d004ae",
   "metadata": {},
   "source": [
    "## Step 2: Analyze the Unoptimized Table\n",
    "\n",
    "Before we apply any optimizations, let's run a query on our raw table and establish a baseline for performance. We'll look for sales data for a specific customer and product.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Run the query below.\n",
    "2. Open the Spark UI for the job that just ran.\n",
    "3. Observe the number of files read and the time taken for the query to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline query â€” read from saved Delta table\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_raw = (spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\")\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      )\n",
    "df_raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57316fef",
   "metadata": {},
   "source": [
    "## Step 3: Implementing Partitioning\n",
    "\n",
    "Partitioning is a way to divide a table into smaller, more manageable parts based on the values of one or more columns. This is most effective on columns with low cardinality that are frequently used in filters.\n",
    "\n",
    "Let's partition our sales data by `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2249ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a partitioned table\n",
    "(df_raw.write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"country\")\n",
    " .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_partitioned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfccd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same query on the partitioned table\n",
    "(spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_partitioned\")\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce1725",
   "metadata": {},
   "source": [
    "**Analysis of Partitioning:**\n",
    "\n",
    "Go to the Spark UI and compare the execution plan with the baseline. You'll notice that if a filter on the partition key (`country`) was present, Spark would be able to prune entire directories, significantly reducing the amount of data scanned. Even without a direct filter on the partition key, observe any changes in the query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8bf88",
   "metadata": {},
   "source": [
    "## Step 4: Applying Z-Ordering\n",
    "\n",
    "Z-Ordering is a technique that co-locates related information in the same set of files. This is particularly useful for high-cardinality columns that are often used in query predicates. We will apply Z-Ordering on `customer_id` and `product_id` to our partitioned table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24eea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# IMPORTANT: Do NOT apply Z-Ordering to the already-partitioned table here.\n",
    "# Instead, create a z-ordered copy of the unpartitioned raw table for comparison.\n",
    "# Drop the target table if it exists so this cell is idempotent in iterative runs.\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG_NAME}.{SCHEMA_NAME}.sales_raw_zorder\")\n",
    "\n",
    "# Create a separate table from sales_raw to apply Z-Ordering to (keeps sales_partitioned untouched)\n",
    "spark.sql(f\"CREATE TABLE {CATALOG_NAME}.{SCHEMA_NAME}.sales_raw_zorder USING DELTA AS SELECT * FROM {CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\")\n",
    "\n",
    "# Convert the new table to a DeltaTable object\n",
    "delta_table = DeltaTable.forName(spark, f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw_zorder\")\n",
    "\n",
    "# Apply Z-Ordering on high-cardinality columns for the raw copy\n",
    "delta_table.optimize().executeZOrderBy(\"customer_id\", \"product_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the query to see the effect of Z-Ordering on the z-ordered raw table\n",
    "(spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_raw_zorder\")\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34832b",
   "metadata": {},
   "source": [
    "**Analysis of Z-Ordering:**\n",
    "\n",
    "In the Spark UI, look at the \"Details\" for the scan phase. You should see a significant reduction in the number of files read, demonstrating the effectiveness of data skipping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace5215",
   "metadata": {},
   "source": [
    "## Step 5: Manual Compaction (OPTIMIZE)\n",
    "\n",
    "Over time, streaming and DML operations can create many small files in your Delta table, which can hurt read performance. The `OPTIMIZE` command compacts these small files into larger ones.\n",
    "\n",
    "First, let's simulate the creation of many small files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e668fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate small file creation by writing in a loop\n",
    "for i in range(10):\n",
    "    (df_raw.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_to_compact\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of files\n",
    "location = spark.sql(f\"DESCRIBE DETAIL {CATALOG_NAME}.{SCHEMA_NAME}.sales_to_compact\").select(\"location\").first()[0]\n",
    "display(dbutils.fs.ls(location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, perform manual compaction\n",
    "delta_table_to_compact = DeltaTable.forName(spark, f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_to_compact\")\n",
    "delta_table_to_compact.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1755b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of files again\n",
    "location = spark.sql(f\"DESCRIBE DETAIL {CATALOG_NAME}.{SCHEMA_NAME}.sales_to_compact\").select(\"location\").first()[0]\n",
    "display(dbutils.fs.ls(location))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c727a",
   "metadata": {},
   "source": [
    "**Analysis of Compaction:**\n",
    "\n",
    "- Observe the reduction in the number of files after running `OPTIMIZE`. This leads to more efficient reads as Spark needs to open and process fewer files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6967f",
   "metadata": {},
   "source": [
    "## Step 6: Auto Compaction\n",
    "\n",
    "Databricks can automatically compact small files for you. This is enabled through table properties.\n",
    "\n",
    "Let's create a new table with auto-compaction enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af680a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with auto-compaction enabled\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {CATALOG_NAME}.{SCHEMA_NAME}.sales_auto_compact\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "    'delta.autoOptimize.autoCompact' = 'true'\n",
    ")\n",
    "AS SELECT * FROM {CATALOG_NAME}.{SCHEMA_NAME}.sales_raw\n",
    "\"\"\")\n",
    "\n",
    "# Simulate small writes to this table\n",
    "for i in range(10):\n",
    "    (df.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_auto_compact\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc02aa9",
   "metadata": {},
   "source": [
    "**Analysis of Auto Compaction:**\n",
    "\n",
    "- After the writes complete, query the table's history to see the `OPTIMIZE` operations that were automatically triggered. Auto-compaction helps maintain optimal file sizes without manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05036473",
   "metadata": {},
   "source": [
    "## Step 7: Liquid Clustering\n",
    "\n",
    "Liquid Clustering is a more flexible and adaptive way to organize data compared to partitioning and Z-Ordering. It's especially useful for tables with high-cardinality columns or evolving query patterns.\n",
    "\n",
    "**Note:** Liquid Clustering requires a Databricks Runtime that supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09965269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with Liquid Clustering\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {CATALOG_NAME}.{SCHEMA_NAME}.sales_liquid_clustered (\n",
    "  transaction_id STRING,\n",
    "  customer_id INT,\n",
    "  product_id INT,\n",
    "  sale_date DATE,\n",
    "  quantity INT,\n",
    "  unit_price DOUBLE,\n",
    "  country STRING\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY (customer_id, product_id)\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into the clustered table\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_liquid_clustered\")\n",
    "\n",
    "# Rerun the query on the liquid clustered table\n",
    "(spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_liquid_clustered\")\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8426655",
   "metadata": {},
   "source": [
    "**Analysis of Liquid Clustering:**\n",
    "\n",
    "1.  Run the query on the liquid clustered table.\n",
    "2.  Examine the Spark UI. Liquid Clustering provides similar data skipping benefits as Z-Ordering but is more adaptive to changes in your data and queries over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060e6c8",
   "metadata": {},
   "source": [
    "## Step 8: VACUUM\n",
    "\n",
    "The `VACUUM` command removes old, unreferenced data files from your Delta table's storage. This is crucial for managing storage costs and cleaning up your data lake.\n",
    "\n",
    "**Important:** By default, `VACUUM` has a retention period of 7 days to prevent accidental deletion of data that might still be in use. For this educational exercise, we will disable this check. **Do not do this in a production environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ec0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the retention period check for demonstration purposes\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# VACUUM the partitioned table\n",
    "delta_table_to_vacuum = DeltaTable.forName(spark, f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_partitioned\")\n",
    "delta_table_to_vacuum.vacuum()\n",
    "\n",
    "# Re-enable the retention check\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd14ef4",
   "metadata": {},
   "source": [
    "**Analysis of VACUUM:**\n",
    "\n",
    "- The `VACUUM` command cleans up files that are no longer part of the current version of the table. You can check the file system before and after running `VACUUM` on a table that has undergone several modifications to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204a988",
   "metadata": {},
   "source": [
    "## Project Conclusion\n",
    "\n",
    "In this project, we have explored several key techniques for optimizing Delta Lake tables in Databricks. We've seen how partitioning, Z-Ordering, compaction, and Liquid Clustering can significantly improve query performance by reducing the amount of data that needs to be scanned. We also learned how to maintain our tables using the `VACUUM` command.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **Partitioning:** Best for low-cardinality columns used frequently in filters.\n",
    "- **Z-Ordering:** Effective for improving data skipping on high-cardinality columns.\n",
    "- **Compaction:** Solves the \"small file problem\" and is crucial for read performance.\n",
    "- **Liquid Clustering:** A modern, flexible approach to data layout that adapts to your data and queries.\n",
    "- **VACUUM:** Essential for managing storage and cleaning up old data files.\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Run the following cell to drop the catalog and all associated tables created during this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the catalog and all its contents\n",
    "spark.sql(f\"DROP CATALOG IF EXISTS {CATALOG_NAME} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
