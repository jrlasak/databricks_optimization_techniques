{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb54111",
   "metadata": {},
   "source": [
    "# Delta Lake Optimization Project\n",
    "\n",
    "This notebook provides a hands-on guide to optimizing Delta Lake tables in Databricks. We will explore and compare various optimization techniques using synthetically generated sales data.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "- Understand and implement table partitioning.\n",
    "- Apply Z-Ordering to improve data skipping.\n",
    "- Perform manual and automatic compaction to optimize file sizes.\n",
    "- Utilize Liquid Clustering for flexible data layout.\n",
    "- Use the VACUUM command to manage table storage.\n",
    "- Analyze Spark UI to observe execution plans and performance improvements.\n",
    "\n",
    "**Setup:**\n",
    "First, we'll define the catalog and schema for our project. Make sure you have the necessary permissions to create catalogs and schemas in your Databricks workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG_NAME = \"delta_optimization_project\"\n",
    "SCHEMA_NAME = \"sales_data\"\n",
    "\n",
    "# Create the catalog and schema if they don't exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818a1c6",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Sales Data\n",
    "\n",
    "We'll start by creating a synthetic dataset of sales transactions. This data will be the foundation for our optimization experiments. We'll include a variety of data types and cardinalities to make our tests realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Define the number of records\n",
    "num_records = 5_000_000\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(num_records):\n",
    "    data = []\n",
    "    for _ in range(num_records):\n",
    "        data.append({\n",
    "            \"transaction_id\": fake.uuid4(),\n",
    "            \"customer_id\": random.randint(1000, 2000),\n",
    "            \"product_id\": random.randint(100, 500),\n",
    "            \"sale_date\": fake.date_between(start_date=\"-2y\", end_date=\"today\"),\n",
    "            \"quantity\": random.randint(1, 10),\n",
    "            \"unit_price\": round(random.uniform(10.5, 200.5), 2),\n",
    "            \"country\": fake.country()\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "pdf = pd.DataFrame(generate_data(num_records))\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Write the data to a base Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d004ae",
   "metadata": {},
   "source": [
    "## Step 2: Analyze the Unoptimized Table\n",
    "\n",
    "Before we apply any optimizations, let's run a query on our raw table and establish a baseline for performance. We'll look for sales data for a specific customer and product.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Run the query below.\n",
    "2. Open the Spark UI for the job that just ran.\n",
    "3. Observe the number of files read and the time taken for the query to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline query\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "(df.where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57316fef",
   "metadata": {},
   "source": [
    "## Step 3: Implementing Partitioning\n",
    "\n",
    "Partitioning is a way to divide a table into smaller, more manageable parts based on the values of one or more columns. This is most effective on columns with low cardinality that are frequently used in filters.\n",
    "\n",
    "Let's partition our sales data by `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2249ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a partitioned table\n",
    "(df.write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"country\")\n",
    " .saveAsTable(\"sales_partitioned\"))\n",
    "\n",
    "# Run the same query on the partitioned table\n",
    "(spark.read.table(\"sales_partitioned\")\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce1725",
   "metadata": {},
   "source": [
    "**Analysis of Partitioning:**\n",
    "\n",
    "1.  Run the query above on the partitioned table.\n",
    "2.  Go to the Spark UI and compare the execution plan with the baseline. You'll notice that if a filter on the partition key (`country`) was present, Spark would be able to prune entire directories, significantly reducing the amount of data scanned. Even without a direct filter on the partition key, observe any changes in the query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8bf88",
   "metadata": {},
   "source": [
    "## Step 4: Applying Z-Ordering\n",
    "\n",
    "Z-Ordering is a technique that co-locates related information in the same set of files. This is particularly useful for high-cardinality columns that are often used in query predicates. We will apply Z-Ordering on `customer_id` and `product_id` to our partitioned table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24eea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# Convert the partitioned table to a DeltaTable object\n",
    "delta_table = DeltaTable.forName(spark, \"sales_partitioned\")\n",
    "\n",
    "# Apply Z-Ordering\n",
    "delta_table.optimize().executeZOrderBy(\"customer_id\", \"product_id\")\n",
    "\n",
    "# Rerun the query to see the effect of Z-Ordering\n",
    "(spark.read.table(\"sales_partitioned\")\n",
    "      .where((col(\"customer_id\") == 1500) & (col(\"product_id\") == 250))\n",
    "      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34832b",
   "metadata": {},
   "source": [
    "**Analysis of Z-Ordering:**\n",
    "\n",
    "1.  After running the `OPTIMIZE` command with `ZORDER BY`, re-run the query.\n",
    "2.  In the Spark UI, look at the \"Details\" for the scan phase. You should see a significant reduction in the number of files read, demonstrating the effectiveness of data skipping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace5215",
   "metadata": {},
   "source": [
    "## Step 5: Manual Compaction (OPTIMIZE)\n",
    "\n",
    "Over time, streaming and DML operations can create many small files in your Delta table, which can hurt read performance. The `OPTIMIZE` command compacts these small files into larger ones.\n",
    "\n",
    "First, let's simulate the creation of many small files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e668fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate small file creation by writing in a loop\n",
    "for i in range(10):\n",
    "    (df.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(\"sales_to_compact\"))\n",
    "\n",
    "# Check the number of files\n",
    "display(dbutils.fs.ls(f\"dbfs:/user/hive/warehouse/{SCHEMA_NAME}.db/sales_to_compact\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, perform manual compaction\n",
    "delta_table_to_compact = DeltaTable.forName(spark, \"sales_to_compact\")\n",
    "delta_table_to_compact.optimize().executeCompaction()\n",
    "\n",
    "# Check the number of files again\n",
    "display(dbutils.fs.ls(f\"dbfs:/user/hive/warehouse/{SCHEMA_NAME}.db/sales_to_compact\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c727a",
   "metadata": {},
   "source": [
    "**Analysis of Compaction:**\n",
    "\n",
    "- Observe the reduction in the number of files after running `OPTIMIZE`. This leads to more efficient reads as Spark needs to open and process fewer files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6967f",
   "metadata": {},
   "source": [
    "## Step 6: Auto Compaction\n",
    "\n",
    "Databricks can automatically compact small files for you. This is enabled through table properties.\n",
    "\n",
    "Let's create a new table with auto-compaction enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af680a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with auto-compaction enabled\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE sales_auto_compact\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "    'delta.autoOptimize.autoCompact' = 'true'\n",
    ")\n",
    "AS SELECT * FROM sales_raw\n",
    "\"\"\")\n",
    "\n",
    "# Simulate small writes to this table\n",
    "for i in range(10):\n",
    "    (df.sample(fraction=0.001)\n",
    "     .write.format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .saveAsTable(\"sales_auto_compact\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc02aa9",
   "metadata": {},
   "source": [
    "**Analysis of Auto Compaction:**\n",
    "\n",
    "- After the writes complete, query the table's history to see the `OPTIMIZE` operations that were automatically triggered. Auto-compaction helps maintain optimal file sizes without manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05036473",
   "metadata": {},
   "source": [
    "## Step 7: Liquid Clustering\n",
    "\n",
    "Liquid Clustering is a more flexible and adaptive way to organize data compared to partitioning and Z-Ordering. It's especially useful for tables with high-cardinality columns or evolving query patterns.\n",
    "\n",
    "**Note:** Liquid Clustering requires a Databricks Runtime that supports it."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
